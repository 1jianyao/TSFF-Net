# YOLOv5 ğŸš€ by Ultralytics, AGPL-3.0 license
"""
Common modules
"""

import ast
import contextlib
import json
import math
import platform
import warnings
import zipfile
from collections import OrderedDict, namedtuple
from copy import copy
from pathlib import Path
from urllib.parse import urlparse

import cv2
import numpy as np
import pandas as pd
import requests
import torch
import torch.nn as nn
from PIL import Image
from torch.cuda import amp

from utils import TryExcept
from utils.dataloaders import exif_transpose, letterbox
from utils.general import (LOGGER, ROOT, Profile, check_requirements, check_suffix, check_version, colorstr,
                           increment_path, is_jupyter, make_divisible, non_max_suppression, scale_boxes, xywh2xyxy,
                           xyxy2xywh, yaml_load)
from utils.plots import Annotator, colors, save_one_box
from utils.torch_utils import copy_attr, smart_inference_mode


# ä¸ºsameå·ç§¯æˆ–sameæ± åŒ–è‡ªåŠ¨æ‰©å……
def autopad(k, p=None, d=1):  # kernel, padding, dilation
    # Pad to 'same' shape outputs
    if d > 1:
        k = d * (k - 1) + 1 if isinstance(k, int) else [d * (x - 1) + 1 for x in k]  # actual kernel-size
    if p is None:
        p = k // 2 if isinstance(k, int) else [x // 2 for x in k]  # auto-pad
    return p


# æ ‡å‡†å·ç§¯ï¼šconv+BN+hardswish
class Conv(nn.Module):
    # Standard convolution with args(ch_in, ch_out, kernel, stride, padding, groups, dilation, activation)
    default_act = nn.SiLU()  # default activation

    def __init__(self, c1, c2, k=1, s=1, p=None, g=1, d=1, act=True):
        super().__init__()
        self.conv = nn.Conv2d(c1, c2, k, s, autopad(k, p, d), groups=g, dilation=d, bias=False)
        self.bn = nn.BatchNorm2d(c2)
        self.act = self.default_act if act is True else act if isinstance(act, nn.Module) else nn.Identity()

    def forward(self, x):
        # y1=self.conv(x)
        # y1=self.bn(y1)
        # y1=self.act(y1)
        return self.act(self.bn(self.conv(x)))

    def forward_fuse(self, x):
        # print("å‰",x.shape)
        # y=self.act(self.conv(x))
        # print("å",y.shape)
        return self.act(self.conv(x))


# æ·±åº¦å¯åˆ†ç¦»å·ç§¯
class DWConv(Conv):
    # Depth-wise convolution
    def __init__(self, c1, c2, k=1, s=1, d=1, act=True):  # ch_in, ch_out, kernel, stride, dilation, activation
        super().__init__(c1, c2, k, s, g=math.gcd(c1, c2), d=d, act=act)


class DWConvTranspose2d(nn.ConvTranspose2d):
    # Depth-wise transpose convolution
    def __init__(self, c1, c2, k=1, s=1, p1=0, p2=0):  # ch_in, ch_out, kernel, stride, padding, padding_out
        super().__init__(c1, c2, k, s, p1, p2, groups=math.gcd(c1, c2))


class TransformerLayer(nn.Module):
    # Transformer layer https://arxiv.org/abs/2010.11929 (LayerNorm layers removed for better performance)
    def __init__(self, c, num_heads):
        super().__init__()
        self.q = nn.Linear(c, c, bias=False)
        self.k = nn.Linear(c, c, bias=False)
        self.v = nn.Linear(c, c, bias=False)
        self.ma = nn.MultiheadAttention(embed_dim=c, num_heads=num_heads)
        self.fc1 = nn.Linear(c, c, bias=False)
        self.fc2 = nn.Linear(c, c, bias=False)

    def forward(self, x):
        x = self.ma(self.q(x), self.k(x), self.v(x))[0] + x
        x = self.fc2(self.fc1(x)) + x
        return x


class TransformerBlock(nn.Module):
    # Vision Transformer https://arxiv.org/abs/2010.11929
    def __init__(self, c1, c2, num_heads, num_layers):
        super().__init__()
        self.conv = None
        if c1 != c2:
            self.conv = Conv(c1, c2)
        self.linear = nn.Linear(c2, c2)  # learnable position embedding
        self.tr = nn.Sequential(*(TransformerLayer(c2, num_heads) for _ in range(num_layers)))
        self.c2 = c2

    def forward(self, x):
        if self.conv is not None:
            x = self.conv(x)
        b, _, w, h = x.shape
        p = x.flatten(2).permute(2, 0, 1)
        return self.tr(p + self.linear(p)).permute(1, 2, 0).reshape(b, self.c2, w, h)


# ä¸¤ä¸ªå·ç§¯convæˆ–è€…ä¸¤ä¸ªå·ç§¯åŠ ä¸Šx
class Bottleneck(nn.Module):
    # Standard bottleneck
    def __init__(self, c1, c2, shortcut=True, g=1, e=0.5):  # ch_in, ch_out, shortcut, groups, expansion
        super().__init__()
        c_ = int(c2 * e)  # hidden channels
        self.cv1 = Conv(c1, c_, 1, 1)
        self.cv2 = Conv(c_, c2, 3, 1, g=g)
        self.add = shortcut and c1 == c2

    def forward(self, x):
        return x + self.cv2(self.cv1(x)) if self.add else self.cv2(self.cv1(x))


class BottleneckCSP(nn.Module):
    # CSP Bottleneck https://github.com/WongKinYiu/CrossStagePartialNetworks
    def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5):  # ch_in, ch_out, number, shortcut, groups, expansion
        super().__init__()
        c_ = int(c2 * e)  # hidden channels
        self.cv1 = Conv(c1, c_, 1, 1)
        self.cv2 = nn.Conv2d(c1, c_, 1, 1, bias=False)
        self.cv3 = nn.Conv2d(c_, c_, 1, 1, bias=False)
        self.cv4 = Conv(2 * c_, c2, 1, 1)
        self.bn = nn.BatchNorm2d(2 * c_)  # applied to cat(cv2, cv3)
        self.act = nn.SiLU()
        self.m = nn.Sequential(*(Bottleneck(c_, c_, shortcut, g, e=1.0) for _ in range(n)))

    def forward(self, x):
        y1 = self.cv3(self.m(self.cv1(x)))
        y2 = self.cv2(x)
        return self.cv4(self.act(self.bn(torch.cat((y1, y2), 1))))


class CrossConv(nn.Module):
    # Cross Convolution Downsample
    def __init__(self, c1, c2, k=3, s=1, g=1, e=1.0, shortcut=False):
        # ch_in, ch_out, kernel, stride, groups, expansion, shortcut
        super().__init__()
        c_ = int(c2 * e)  # hidden channels
        self.cv1 = Conv(c1, c_, (1, k), (1, s))
        self.cv2 = Conv(c_, c2, (k, 1), (s, 1), g=g)
        self.add = shortcut and c1 == c2

    def forward(self, x):
        return x + self.cv2(self.cv1(x)) if self.add else self.cv2(self.cv1(x))


class C3(nn.Module):
    # CSP Bottleneck with 3 convolutions
    def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5):  # ch_in, ch_out, number, shortcut, groups, expansion
        super().__init__()
        c_ = int(c2 * e)  # hidden channels
        self.cv1 = Conv(c1, c_, 1, 1)
        self.cv2 = Conv(c1, c_, 1, 1)
        self.cv3 = Conv(2 * c_, c2, 1)  # optional act=FReLU(c2)
        self.m = nn.Sequential(*(Bottleneck(c_, c_, shortcut, g, e=1.0) for _ in range(n)))

    def forward(self, x):
        return self.cv3(torch.cat((self.m(self.cv1(x)), self.cv2(x)), 1))


class C3x(C3):
    # C3 module with cross-convolutions
    def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5):
        super().__init__(c1, c2, n, shortcut, g, e)
        c_ = int(c2 * e)
        self.m = nn.Sequential(*(CrossConv(c_, c_, 3, 1, g, 1.0, shortcut) for _ in -range(n)))


class C3TR(C3):
    # C3 module with TransformerBlock()
    def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5):
        super().__init__(c1, c2, n, shortcut, g, e)
        c_ = int(c2 * e)
        self.m = TransformerBlock(c_, c_, 4, n)


class C3SPP(C3):
    # C3 module with SPP()
    def __init__(self, c1, c2, k=(5, 9, 13), n=1, shortcut=True, g=1, e=0.5):
        super().__init__(c1, c2, n, shortcut, g, e)
        c_ = int(c2 * e)
        self.m = SPP(c_, c_, k)


class C3Ghost(C3):
    # C3 module with GhostBottleneck()
    def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5):
        super().__init__(c1, c2, n, shortcut, g, e)
        c_ = int(c2 * e)  # hidden channels
        self.m = nn.Sequential(*(GhostBottleneck(c_, c_) for _ in range(n)))


class SPP(nn.Module):
    # Spatial Pyramid Pooling (SPP) layer https://arxiv.org/abs/1406.4729
    def __init__(self, c1, c2, k=(5, 9, 13)):
        super().__init__()
        c_ = c1 // 2  # hidden channels
        self.cv1 = Conv(c1, c_, 1, 1)
        self.cv2 = Conv(c_ * (len(k) + 1), c2, 1, 1)
        self.m = nn.ModuleList([nn.MaxPool2d(kernel_size=x, stride=1, padding=x // 2) for x in k])

    def forward(self, x):
        x = self.cv1(x)
        with warnings.catch_warnings():
            warnings.simplefilter('ignore')  # suppress torch 1.9.0 max_pool2d() warning
            return self.cv2(torch.cat([x] + [m(x) for m in self.m], 1))


class SPPF(nn.Module):
    # Spatial Pyramid Pooling - Fast (SPPF) layer for YOLOv5 by Glenn Jocher
    def __init__(self, c1, c2, k=5):  # equivalent to SPP(k=(5, 9, 13))
        super().__init__()
        c_ = c1 // 2  # hidden channels
        self.cv1 = Conv(c1, c_, 1, 1)
        self.cv2 = Conv(c_ * 4, c2, 1, 1)
        self.m = nn.MaxPool2d(kernel_size=k, stride=1, padding=k // 2)

    def forward(self, x):
        x = self.cv1(x)
        with warnings.catch_warnings():
            warnings.simplefilter('ignore')  # suppress torch 1.9.0 max_pool2d() warning
            y1 = self.m(x)
            y2 = self.m(y1)
            return self.cv2(torch.cat((x, y1, y2, self.m(y2)), 1))


class Focus(nn.Module):
    # Focus wh information into c-space
    def __init__(self, c1, c2, k=1, s=1, p=None, g=1, act=True):  # ch_in, ch_out, kernel, stride, padding, groups
        super().__init__()
        self.conv = Conv(c1 * 4, c2, k, s, p, g, act=act)
        # self.contract = Contract(gain=2)

    def forward(self, x):  # x(b,c,w,h) -> y(b,4c,w/2,h/2)
        return self.conv(torch.cat((x[..., ::2, ::2], x[..., 1::2, ::2], x[..., ::2, 1::2], x[..., 1::2, 1::2]), 1))
        # return self.conv(self.contract(x))


class GhostConv(nn.Module):
    # Ghost Convolution https://github.com/huawei-noah/ghostnet
    def __init__(self, c1, c2, k=1, s=1, g=1, act=True):  # ch_in, ch_out, kernel, stride, groups
        super().__init__()
        c_ = c2 // 2  # hidden channels
        self.cv1 = Conv(c1, c_, k, s, None, g, act=act)
        self.cv2 = Conv(c_, c_, 5, 1, None, c_, act=act)

    def forward(self, x):
        y = self.cv1(x)
        return torch.cat((y, self.cv2(y)), 1)


class GhostBottleneck(nn.Module):
    # Ghost Bottleneck https://github.com/huawei-noah/ghostnet
    def __init__(self, c1, c2, k=3, s=1):  # ch_in, ch_out, kernel, stride
        super().__init__()
        c_ = c2 // 2
        self.conv = nn.Sequential(
            GhostConv(c1, c_, 1, 1),  # pw
            DWConv(c_, c_, k, s, act=False) if s == 2 else nn.Identity(),  # dw
            GhostConv(c_, c2, 1, 1, act=False))  # pw-linear
        self.shortcut = nn.Sequential(DWConv(c1, c1, k, s, act=False), Conv(c1, c2, 1, 1,
                                                                            act=False)) if s == 2 else nn.Identity()

    def forward(self, x):
        return self.conv(x) + self.shortcut(x)


class Contract(nn.Module):
    # Contract width-height into channels, i.e. x(1,64,80,80) to x(1,256,40,40)
    def __init__(self, gain=2):
        super().__init__()
        self.gain = gain

    def forward(self, x):
        b, c, h, w = x.size()  # assert (h / s == 0) and (W / s == 0), 'Indivisible gain'
        s = self.gain
        x = x.view(b, c, h // s, s, w // s, s)  # x(1,64,40,2,40,2)
        x = x.permute(0, 3, 5, 1, 2, 4).contiguous()  # x(1,2,2,64,40,40)
        return x.view(b, c * s * s, h // s, w // s)  # x(1,256,40,40)


class Expand(nn.Module):
    # Expand channels into width-height, i.e. x(1,64,80,80) to x(1,16,160,160)
    def __init__(self, gain=2):
        super().__init__()
        self.gain = gain

    def forward(self, x):
        b, c, h, w = x.size()  # assert C / s ** 2 == 0, 'Indivisible gain'
        s = self.gain
        x = x.view(b, s, s, c // s ** 2, h, w)  # x(1,2,2,16,80,80)
        x = x.permute(0, 3, 4, 1, 5, 2).contiguous()  # x(1,16,80,2,80,2)
        return x.view(b, c // s ** 2, h * s, w * s)  # x(1,16,160,160)


class Concat(nn.Module):
    # Concatenate a list of tensors along dimension
    def __init__(self, dimension=1):
        super().__init__()
        self.d = dimension

    def forward(self, x):
        return torch.cat(x, self.d)


class Concat2(nn.Module):
    # Concatenate a list of tensors along dimension
    def __init__(self, dimension=1):
        super().__init__()
        self.d = dimension

    def forward(self, x):
        return torch.cat(x, self.d)


# è‡ªé€‚åº”èåˆæ¨¡å—
class Concat3(nn.Module):
    # Concatenate a list of tensors along dimension
    def __init__(self, c1, c2, ratio=16, kernel_size=7, dimension=1):
        super().__init__()
        self.d = dimension  # æ²¿ç€å“ªä¸ªç»´åº¦è¿›è¡Œæ‹¼æ¥
        self.spatial_attention = SpatialAttention(7)
        self.channel_attention = ChannelAttention(c1, ratio)

    # æ³¨x1ä¸ºåŠ¨æ€å›¾åƒï¼Œx2ä¸ºçº¢å¤–å›¾åƒ
    def forward(self, x1, x2):
        # ç©ºé—´æ³¨æ„åŠ›ç”Ÿæˆæƒé‡
        weight1 = self.spatial_attention(x1)
        weight2 = self.spatial_attention(x2)
        # print("weight1",weight1)
        # print("weight2",weight2)
        weight = (weight1 / weight2)
        # print("weights",weight)
        # ä¸ºä»€ä¹ˆä¸èƒ½å®ç°å¤æ‚åœºæ™¯ä¸‹çš„è‡ªé€‚åº”ï¼ˆéƒ¨åˆ†è‡ªé€‚åº”ï¼‰
        x2 = weight * x2
        x1 = x1 * (2 - weight)
        # ä¸ºä»€ä¹ˆæ˜¯catæ“ä½œè€Œä¸æ˜¯addæ“ä½œï¼ˆä¸ºä»€ä¹ˆï¼‰
        x = torch.cat((x1, x2), self.d)
        # è¿™é‡Œè¿›è¡Œä¿®æ”¹ï¼ŒXåº”è¯¥æ˜¯å†™é”™äº†
        # X = self.channel_attention(x)
        # print("è¾“å‡ºçš„å¤§å°",x.shape)
        # print("è¾“å‡ºçš„é€šé“æ³¨æ„åŠ›æƒé‡å¤§å°",X.shape)
        X_w = self.channel_attention(x)
        # print("X_W",X_w)
        return x * X_w


class AC(nn.Module):
    def __init__(self, dim, reduction_ratio=32):
        super().__init__()
        self.ac = nn.Sequential(
            nn.AdaptiveAvgPool2d(1),
            nn.Conv2d(dim, dim // reduction_ratio, 1),
            nn.Conv2d(dim // reduction_ratio, dim, 1),
            nn.BatchNorm2d(dim)
        )

    def forward(self, x):
        return self.ac(x)


"""
dim è¾“å…¥é€šé“
reducition_rationå˜æ¢å‹ç¼©çš„é€šé“æ•°
"""


class ALS(nn.Module):
    def __init__(self, dim, reduction_ratio=32):
        super().__init__()
        self.als = nn.Sequential(
            nn.Conv2d(dim, dim // reduction_ratio, 1),
            nn.Conv2d(dim // reduction_ratio, dim // reduction_ratio, 3, 1, 1),
            nn.Conv2d(dim // reduction_ratio, dim, 1),
            nn.BatchNorm2d(dim)
        )

    def forward(self, x):
        return self.als(x)


# class BA2M(nn.Module):
#     def __init__(self, dim, reduction_ratio=32, attn_drop=0., proj_drop=0.):
#         super().__init__()
#
#         self.ac = AC(dim, reduction_ratio)
#         self.als = ALS(dim, reduction_ratio)
#         # self.ags = AGS(dim, reduction_ratio, attn_drop, proj_drop)
#
#         self.avgpool = nn.AdaptiveAvgPool2d(1)
#         # ä¸€å®šè¦æœ‰dim=0
#         self.softmax = nn.Softmax(dim=0)
#         self.apply(self.init_weight)
#
#     #
#     def init_weight(self, m):
#         # print("m",m)
#         if isinstance(m, nn.BatchNorm2d):
#             nn.init.zeros_(m.bias)
#             nn.init.ones_(m.weight)
#         elif isinstance(m, (nn.Linear, nn.Conv2d)):
#             nn.init.xavier_normal_(m.weight.data)
#
#     def forward(self, x):
#
#         ac = self.ac(x)
#         als = self.avgpool(self.als(x))
#         # ags = self.avgpool(self.ags(x))
#
#         attn = torch.maximum(ac, als)
#         # attn = torch.maximum(attn, ags)
#         attn = torch.mean(attn, axis=1, keepdim=True)
#         attn = self.softmax(attn)
#         print(attn)
#         # x=[x1,x2],x1ä¸ºåŠ¨æ€å›¾åƒï¼Œx2ä¸ºçº¢å¤–å›¾åƒ
#         out = x * attn * 2
#         return out


class Dual_BA2M(nn.Module):
    def __init__(self, dim, reduction_ratio=32, attn_drop=0., proj_drop=0.):
        super().__init__()
        self.ba2m = BA2M(dim=dim, reduction_ratio=reduction_ratio, attn_drop=attn_drop, proj_drop=proj_drop)
    # æ³¨x1ä¸ºåŠ¨æ€å›¾åƒï¼Œx2ä¸ºçº¢å¤–å›¾åƒ
    def forward(self, x1, x2):
        for i in range(x1.shape[0]):
            output_x1 = torch.cat([x1[i:i + 1], x2[i:i + 1]], dim=0)
            output_x2 = self.ba2m(output_x1)
            output_x3 = torch.cat([output_x2[0:1], output_x2[1:2]], dim=1)
            if i == 0:
                output_x = output_x3
            else:
                output_x = torch.cat([output_x, output_x3], dim=0)
        return output_x
class AGS(nn.Module):
    def __init__(self, dim, reduction_ratio=32, attn_drop=0., proj_drop=0.):
        super().__init__()
        self.reduction_ratio = reduction_ratio
        self.qkv = nn.Linear(dim // reduction_ratio, dim // reduction_ratio * 3)
        # ä¸€å®šè¦æœ‰dim=,
        self.softmax = nn.Softmax(dim=0)
        self.attn_drop = nn.Dropout(attn_drop)

    def forward(self, x):
        # è·å–Bã€Cã€Hã€W
        B, C, H, W = x.shape
        # print("x.shape", x.shape)
        # å±•
        # x = torch.flatten(x, 2).transpose([0, 2, 1]).reshape((B, H * W, self.reduction_ratio, -1)) \
        #     .transpose([0, 2, 1, 3])  # B R H*W C' ,C' = C // R

        # åå®½é«˜ç»´åº¦ç›¸ä¹˜
        x = torch.reshape(x, (B, C, H * W))
        # print("åå®½é«˜ç»´åº¦ç›¸ä¹˜",x.shape)
        # åä¸¤ç»´åº¦äº¤æ¢
        x = x.permute(0, 2, 1)
        # print("åä¸¤ç»´åº¦äº¤æ¢", x.shape)
        # é‡æ–°å®šä¹‰å¤§å°
        x = x.reshape((B, H * W, self.reduction_ratio, -1))
        # print("é‡æ–°å®šä¹‰å¤§å°", x.shape)
        # æœ€åä¸­é—´ä¸¤ç»´äº¤æ¢ä½ç½®
        x = x.permute(0, 2, 1, 3)
        # print("æœ€åä¸­é—´ä¸¤ç»´äº¤æ¢ä½ç½®", x.shape)

        qkv = self.qkv(x)
        q, k, v = torch.chunk(qkv, chunks=3, axis=-1)
        # print("q",q.shape)
        # print("k",k.shape)
        # print("v",v.shape)
        #
        # attn = q @ k.transpose([0, 1, 3, 2])
        attn = q @ k.permute(0, 1, 3, 2)
        # print("ä½¿ç”¨å‰",attn.shape)
        attn = self.softmax(attn)
        # print("ä½¿ç”¨å", attn.shape)
        attn = self.attn_drop(attn)
        #
        # out = (attn @ v).transpose([0, 2, 1, 3]).reshape((B, H * W, -1)).transpose([0, 1, 2]) \
        # .reshape((B, -1, H, W))
        out = (attn @ v).permute(0, 2, 1, 3).reshape((B, H * W, -1)).permute(0, 1, 2) \
            .reshape((B, -1, H, W))

        return out


class BA2M(nn.Module):
    def __init__(self, dim, reduction_ratio=32, attn_drop=0., proj_drop=0.):
        super().__init__()

        self.ac = AC(dim, reduction_ratio)

        self.als = ALS(dim, reduction_ratio)
        self.ags = AGS(dim, reduction_ratio, attn_drop, proj_drop)

        self.avgpool = nn.AdaptiveAvgPool2d(1)
        # ä¸€å®šè¦æœ‰dim=0
        self.softmax = nn.Softmax(dim=0)

        self.apply(self.init_weight)

    def init_weight(self, m):
        # print("m",m)
        if isinstance(m, nn.BatchNorm2d):
            nn.init.zeros_(m.bias)
            nn.init.ones_(m.weight)
        elif isinstance(m, (nn.Linear, nn.Conv2d)):
            nn.init.xavier_normal_(m.weight.data)

    def forward(self, x):

        ac = self.ac(x)
        # print(ac)
        als = self.avgpool(self.als(x))
        # print(als)
        ags = self.avgpool(self.ags(x))

        attn = torch.maximum(ac, als)
        # print(attn)
        attn = torch.maximum(attn, ags)
        attn = torch.mean(attn, axis=1, keepdim=True)
        # print("attn",attn.shape)
        attn = self.softmax(attn)
        # print(attn)
        out = x * attn

        return out


class Multi_BA2M(nn.Module):
    def __init__(self, dim, reduction_ratio=32, attn_drop=0., proj_drop=0.):
        super().__init__()
        self.ba2m = BA2M(dim=dim, reduction_ratio=reduction_ratio, attn_drop=attn_drop, proj_drop=proj_drop)

    # æ³¨x1ä¸ºåŠ¨æ€å›¾åƒï¼Œx2ä¸ºçº¢å¤–å›¾åƒ
    def forward(self, x1, x2):
        # x = []
        # w = []
        for i in range(x1.shape[0]):
            output_x1 = torch.cat([x1[i:i + 1], x2[i:i + 1]], dim=0)
            # print("output1",output_x1.shape)
            output_x2 = self.ba2m(output_x1)
            # print("output2",output_x2.shape)
            output_x3 = torch.cat([output_x2[0:1], output_x2[1:2]], dim=1)
            # print("output3", output_x3.shape)
            if i == 0:
                output_x = output_x3
            else:
                output_x = torch.cat([output_x, output_x3], dim=0)
        return output_x



########################################################
###   1
class BA2M_c(nn.Module):
    def __init__(self, dim, reduction_ratio=32, attn_drop=0., proj_drop=0.):
        super().__init__()
        self.ac = AC(dim, reduction_ratio)
        self.avgpool = nn.AdaptiveAvgPool2d(1)
        # ä¸€å®šè¦æœ‰dim=0
        self.softmax = nn.Softmax(dim=0)
        self.apply(self.init_weight)
    def init_weight(self, m):
        # print("m",m)
        if isinstance(m, nn.BatchNorm2d):
            nn.init.zeros_(m.bias)
            nn.init.ones_(m.weight)
        elif isinstance(m, (nn.Linear, nn.Conv2d)):
            nn.init.xavier_normal_(m.weight.data)
    def forward(self, x):
        # B*C*1*1
        ac = self.ac(x)
        attn = torch.mean(ac, axis=1, keepdim=True)
        # B*1*1*1
        attn = self.softmax(attn)
        # print("attn",attn)
        out = x * attn
        return out
class Dual_BA2M_c(nn.Module):
    def __init__(self, dim, reduction_ratio=32, attn_drop=0., proj_drop=0.):
        super().__init__()
        self.ba2m = BA2M_c(dim=dim, reduction_ratio=reduction_ratio, attn_drop=attn_drop, proj_drop=proj_drop)
    # æ³¨x1ä¸ºåŠ¨æ€å›¾åƒï¼Œx2ä¸ºçº¢å¤–å›¾åƒ
    def forward(self, x1, x2):
        # x = []
        # w = []
        for i in range(x1.shape[0]):
            output_x1 = torch.cat([x1[i:i + 1], x2[i:i + 1]], dim=0)
            # print("output1",output_x1.shape)
            output_x2 = self.ba2m(output_x1)
            # print("output2",output_x2.shape)
            output_x3 = torch.add(output_x2[0:1], output_x2[1:2])
            # print("output3", output_x3.shape)
            if i == 0:
                output_x = output_x3
            else:
                output_x = torch.cat([output_x, output_x3], dim=0)
        return output_x

class BA2M_s(nn.Module):
    def __init__(self, dim, reduction_ratio=32, attn_drop=0., proj_drop=0.):
        super().__init__()
        self.als = ALS(dim, reduction_ratio)
        # self.ags = AGS(dim, reduction_ratio, attn_drop, proj_drop)
        self.avgpool = nn.AdaptiveAvgPool2d(1)
        # ä¸€å®šè¦æœ‰dim=0
        self.softmax = nn.Softmax(dim=0)
        self.apply(self.init_weight)
    def init_weight(self, m):
        # print("m",m)
        if isinstance(m, nn.BatchNorm2d):
            nn.init.zeros_(m.bias)
            nn.init.ones_(m.weight)
        elif isinstance(m, (nn.Linear, nn.Conv2d)):
            nn.init.xavier_normal_(m.weight.data)
    def forward(self, x):
        als = self.avgpool(self.als(x))
        attn = torch.mean(als, axis=1, keepdim=True)
        # B*1*1*1
        attn = self.softmax(attn)
        # print("attn",attn)
        out = x * attn
        return out
class Dual_BA2M_s(nn.Module):
    def __init__(self, dim, reduction_ratio=32, attn_drop=0., proj_drop=0.):
        super().__init__()
        self.ba2m = BA2M_s(dim=dim, reduction_ratio=reduction_ratio, attn_drop=attn_drop, proj_drop=proj_drop)
    # æ³¨x1ä¸ºåŠ¨æ€å›¾åƒï¼Œx2ä¸ºçº¢å¤–å›¾åƒ
    def forward(self, x1, x2):
        for i in range(x1.shape[0]):
            output_x1 = torch.cat([x1[i:i + 1], x2[i:i + 1]], dim=0)
            # print("output1",output_x1.shape)
            output_x2 = self.ba2m(output_x1)
            # print("output2",output_x2.shape)
            output_x3 = torch.add(output_x2[0:1], output_x2[1:2])
            # print("output3", output_x3.shape)
            if i == 0:
                output_x = output_x3
            else:
                output_x = torch.cat([output_x, output_x3], dim=0)
        return output_x

class BA2M_M(nn.Module):
    def __init__(self, dim, reduction_ratio=32, attn_drop=0., proj_drop=0.):
        super().__init__()
        self.ac = AC(dim, reduction_ratio)
        self.als = ALS(dim, reduction_ratio)
        # self.ags = AGS(dim, reduction_ratio, attn_drop, proj_drop)
        self.avgpool = nn.AdaptiveAvgPool2d(1)
        # ä¸€å®šè¦æœ‰dim=0
        self.softmax = nn.Softmax(dim=0)
        self.apply(self.init_weight)
    def init_weight(self, m):
        # print("m",m)
        if isinstance(m, nn.BatchNorm2d):
            nn.init.zeros_(m.bias)
            nn.init.ones_(m.weight)
        elif isinstance(m, (nn.Linear, nn.Conv2d)):
            nn.init.xavier_normal_(m.weight.data)

    def forward(self, x):
        # B*C*1*1
        ac = self.ac(x)
        # B*C*1*1
        als = self.avgpool(self.als(x))
        # ags = self.avgpool(self.ags(x))
        # B*C*1*1
        attn = torch.maximum(ac, als)
        attn = torch.mean(attn, axis=1, keepdim=True)
        # B*1*1*1
        attn = self.softmax(attn)
        print("attn",attn)
        out = x * attn
        return out


class Dual_BA2M_M(nn.Module):
    def __init__(self, dim, reduction_ratio=32, attn_drop=0., proj_drop=0.):
        super().__init__()
        self.ba2m = BA2M_M(dim=dim, reduction_ratio=reduction_ratio, attn_drop=attn_drop, proj_drop=proj_drop)
    # æ³¨x1ä¸ºåŠ¨æ€å›¾åƒï¼Œx2ä¸ºçº¢å¤–å›¾åƒ
    def forward(self, x1, x2):
        for i in range(x1.shape[0]):
            output_x1 = torch.cat([x1[i:i + 1], x2[i:i + 1]], dim=0)
            # print("output1",output_x1.shape)
            output_x2 = self.ba2m(output_x1)
            # print("output2",output_x2.shape)
            output_x3 = torch.add(output_x2[0:1], output_x2[1:2])
            # print("output3", output_x3.shape)
            if i == 0:
                output_x = output_x3
            else:
                output_x = torch.cat([output_x, output_x3], dim=0)
        return output_x
class Dual_BA2M_M_cat(nn.Module):
    def __init__(self, dim, reduction_ratio=32, attn_drop=0., proj_drop=0.):
        super().__init__()
        self.ba2m = BA2M_M(dim=dim, reduction_ratio=reduction_ratio, attn_drop=attn_drop, proj_drop=proj_drop)
    # æ³¨x1ä¸ºåŠ¨æ€å›¾åƒï¼Œx2ä¸ºçº¢å¤–å›¾åƒ
    def forward(self, x1, x2):
        for i in range(x1.shape[0]):
            output_x1 = torch.cat([x1[i:i + 1], x2[i:i + 1]], dim=0)
            # print("output1",output_x1.shape)
            output_x2 = self.ba2m(output_x1)
            # print("output2",output_x2.shape)
            output_x3 = torch.cat([output_x2[0:1], output_x2[1:2]], dim=1)
            # print("output3", output_x3.shape)
            if i == 0:
                output_x = output_x3
            else:
                output_x = torch.cat([output_x, output_x3], dim=0)
        return output_x

class SE(nn.Module):
    def __init__(self, dim, reduction_ratio=32):
        super().__init__()
        # å±æ€§åˆ†é…
        # å…¨å±€å¹³å‡æ± åŒ–ï¼Œè¾“å‡ºçš„ç‰¹å¾å›¾çš„å®½é«˜=1
        self.avg_pool = nn.AdaptiveAvgPool2d(output_size=1)
        # ç¬¬ä¸€ä¸ªå…¨è¿æ¥å±‚å°†ç‰¹å¾å›¾çš„é€šé“æ•°ä¸‹é™4å€
        self.fc1 = nn.Linear(in_features=dim, out_features=dim // reduction_ratio, bias=False)
        # reluæ¿€æ´»
        self.relu = nn.ReLU()
        # ç¬¬äºŒä¸ªå…¨è¿æ¥å±‚æ¢å¤é€šé“æ•°
        self.fc2 = nn.Linear(in_features=dim // reduction_ratio, out_features=dim, bias=False)
        # sigmoidæ¿€æ´»å‡½æ•°ï¼Œå°†æƒå€¼å½’ä¸€åŒ–åˆ°0-1
        self.sigmoid = nn.Sigmoid()

    def forward(self, inputs):
        # è·å–è¾“å…¥ç‰¹å¾å›¾çš„shape
        b, c, h, w = inputs.shape
        # å…¨å±€å¹³å‡æ± åŒ– [b,c,h,w]==>[b,c,1,1]
        x = self.avg_pool(inputs)
        # ç»´åº¦è°ƒæ•´ [b,c,1,1]==>[b,c]
        x = x.view([b, c])

        # ç¬¬ä¸€ä¸ªå…¨è¿æ¥ä¸‹é™é€šé“ [b,c]==>[b,c//4]
        x = self.fc1(x)
        x = self.relu(x)
        # ç¬¬äºŒä¸ªå…¨è¿æ¥ä¸Šå‡é€šé“ [b,c//4]==>[b,c]
        x = self.fc2(x)
        # å¯¹é€šé“æƒé‡å½’ä¸€åŒ–å¤„ç†
        x = self.sigmoid(x)
        # print("æƒé‡è¾“å‡ºx", x)
        # è°ƒæ•´ç»´åº¦ [b,c]==>[b,c,1,1]
        x = x.view([b, c, 1, 1])

        return x

class BA2M_SE(nn.Module):
    def __init__(self, dim, reduction_ratio=32, attn_drop=0., proj_drop=0.):
        super().__init__()
        self.ac = SE(dim, reduction_ratio)
        self.avgpool = nn.AdaptiveAvgPool2d(1)
        # ä¸€å®šè¦æœ‰dim=0
        self.softmax = nn.Softmax(dim=0)
        self.apply(self.init_weight)


    def init_weight(self, m):
        # print("m",m)
        if isinstance(m, nn.BatchNorm2d):
            nn.init.zeros_(m.bias)
            nn.init.ones_(m.weight)
        elif isinstance(m, (nn.Linear, nn.Conv2d)):
            nn.init.xavier_normal_(m.weight.data)

    def forward(self, x):
        # B*C*1*1
        ac = self.ac(x)
        attn = torch.mean(ac, axis=1, keepdim=True)
        # B*1*1*1
        attn = self.softmax(attn)
        # print("attn",attn)
        out = x * attn
        return out
class Dual_BA2M_SE(nn.Module):
    def __init__(self, dim, reduction_ratio=32, attn_drop=0., proj_drop=0.):
        super().__init__()
        self.ba2m = BA2M_SE(dim=dim, reduction_ratio=reduction_ratio, attn_drop=attn_drop, proj_drop=proj_drop)
    # æ³¨x1ä¸ºåŠ¨æ€å›¾åƒï¼Œx2ä¸ºçº¢å¤–å›¾åƒ
    def forward(self, x1, x2):
        # x = []
        # w = []
        for i in range(x1.shape[0]):
            output_x1 = torch.cat([x1[i:i + 1], x2[i:i + 1]], dim=0)
            # print("output1",output_x1.shape)
            output_x2 = self.ba2m(output_x1)
            # print("output2",output_x2.shape)
            output_x3 = torch.add(output_x2[0:1], output_x2[1:2])
            # print("output3", output_x3.shape)
            if i == 0:
                output_x = output_x3
            else:
                output_x = torch.cat([output_x, output_x3], dim=0)

        return output_x
# è¿‘ä¼¼é€šé“æ³¨æ„åŠ›æ³¨æ„åŠ›æ¨¡å—(2*C*1*1)
class AC_AFF(nn.Module):
    def __init__(self, dim, reduction_ratio=8):
        super().__init__()
        inter_channels = int(dim // reduction_ratio)
        self.ac = nn.Sequential(
            nn.AdaptiveAvgPool2d(1),
            nn.Conv2d(dim, inter_channels, kernel_size=1, stride=1, padding=0),
            nn.BatchNorm2d(inter_channels),
            nn.ReLU(inplace=True),
            nn.Conv2d(inter_channels, dim, kernel_size=1, stride=1, padding=0),
            nn.BatchNorm2d(dim),
        )
    def forward(self, x):
        return self.ac(x)


"""
dim è¾“å…¥é€šé“
reducition_rationå˜æ¢å‹ç¼©çš„é€šé“æ•°
"""
# è¿‘ä¼¼ç©ºé—´æ³¨æ„åŠ›æ¨¡å—(2*C*H*W)
class ALS_AFF(nn.Module):
    def __init__(self, dim, reduction_ratio=8):
        super().__init__()
        inter_channels = int(dim // reduction_ratio)
        self.als =  nn.Sequential(
            nn.Conv2d(dim, inter_channels, kernel_size=1, stride=1, padding=0),
            nn.BatchNorm2d(inter_channels),
            nn.ReLU(inplace=True),
            nn.Conv2d(inter_channels, dim, kernel_size=1, stride=1, padding=0),
            nn.BatchNorm2d(dim),
        )
    def forward(self, x):
        return self.als(x)

class BA2M_AFF(nn.Module):
    def __init__(self, dim, reduction_ratio=8, attn_drop=0., proj_drop=0.):
        super().__init__()
        self.ac = AC_AFF(dim, reduction_ratio)
        self.als = ALS_AFF(dim, reduction_ratio)
        # self.ags = AGS(dim, reduction_ratio, attn_drop, proj_drop)
        self.avgpool = nn.AdaptiveAvgPool2d(1)
        # ä¸€å®šè¦æœ‰dim=0
        self.softmax = nn.Softmax(dim=0)
        self.apply(self.init_weight)

    def init_weight(self, m):
        # print("m",m)
        if isinstance(m, nn.BatchNorm2d):
            nn.init.zeros_(m.bias)
            nn.init.ones_(m.weight)
        elif isinstance(m, (nn.Linear, nn.Conv2d)):
            nn.init.xavier_normal_(m.weight.data)

    def forward(self, x):
        ac = self.ac(x)
        # B*C*1*1
        als = self.avgpool(self.als(x))
        # B*C*1*1
        attn = torch.maximum(ac, als)
        # B*1*1*1
        attn = torch.mean(attn, axis=1, keepdim=True)
        # B*1*1*1
        attn = self.softmax(attn)
        # print(attn)
        out = x * attn
        return out


class Dual_BA2M_AFF(nn.Module):
    def __init__(self, dim, reduction_ratio=8, attn_drop=0., proj_drop=0.):
        super().__init__()
        self.ba2m = BA2M_AFF(dim=dim, reduction_ratio=reduction_ratio, attn_drop=attn_drop, proj_drop=proj_drop)
    # æ³¨x1ä¸ºåŠ¨æ€å›¾åƒï¼Œx2ä¸ºçº¢å¤–å›¾åƒ
    def forward(self, x1, x2):
        for i in range(x1.shape[0]):
            output_x1 = torch.cat([x1[i:i + 1], x2[i:i + 1]], dim=0)
            output_x2 = self.ba2m(output_x1)
            output_x3 = torch.add(output_x2[0:1], output_x2[1:2])
            if i == 0:
                output_x = output_x3
            else:
                output_x = torch.cat([output_x, output_x3], dim=0)
        return output_x


class Dual_BA2M_AFF_cat(nn.Module):
    def __init__(self, dim, reduction_ratio=8, attn_drop=0., proj_drop=0.):
        super().__init__()
        self.ba2m = BA2M_AFF(dim=dim, reduction_ratio=reduction_ratio, attn_drop=attn_drop, proj_drop=proj_drop)
    # æ³¨x1ä¸ºåŠ¨æ€å›¾åƒï¼Œx2ä¸ºçº¢å¤–å›¾åƒ
    def forward(self, x1, x2):
        # x = []
        # w = []
        for i in range(x1.shape[0]):
            output_x1 = torch.cat([x1[i:i + 1], x2[i:i + 1]], dim=0)
            # print("output1",output_x1.shape)
            output_x2 = self.ba2m(output_x1)
            # print("output2",output_x2.shape)
            output_x3 = torch.cat([output_x2[0:1], output_x2[1:2]], dim=1)
            if i == 0:
                output_x = output_x3
            else:
                output_x = torch.cat([output_x, output_x3], dim=0)

        return output_x
#########################################################
class SKConv_dual(nn.Module):
    def __init__(self, in_channels, out_channels, stride=1, M=2, r=16, L=32):
        '''
        :param in_channels:  è¾“å…¥é€šé“ç»´åº¦
        :param out_channels: è¾“å‡ºé€šé“ç»´åº¦   åŸè®ºæ–‡ä¸­ è¾“å…¥è¾“å‡ºé€šé“ç»´åº¦ç›¸åŒ
        :param stride:  æ­¥é•¿ï¼Œé»˜è®¤ä¸º1
        :param M:  åˆ†æ”¯æ•°
        :param r: ç‰¹å¾Zçš„é•¿åº¦ï¼Œè®¡ç®—å…¶ç»´åº¦d æ—¶æ‰€éœ€çš„æ¯”ç‡ï¼ˆè®ºæ–‡ä¸­ ç‰¹å¾S->Z æ˜¯é™ç»´ï¼Œæ•…éœ€è¦è§„å®š é™ç»´çš„ä¸‹ç•Œï¼‰
        :param L:  è®ºæ–‡ä¸­è§„å®šç‰¹å¾Zçš„ä¸‹ç•Œï¼Œé»˜è®¤ä¸º32
        é‡‡ç”¨åˆ†ç»„å·ç§¯ï¼š groups = 32,æ‰€ä»¥è¾“å…¥channelçš„æ•°å€¼å¿…é¡»æ˜¯groupçš„æ•´æ•°å€
        '''
        super(SKConv_dual, self).__init__()
        d = max(in_channels // r, L)  # è®¡ç®—ä»å‘é‡Cé™ç»´åˆ° å‘é‡Z çš„é•¿åº¦d
        self.M = M
        self.out_channels = out_channels
        self.conv = nn.ModuleList()  # æ ¹æ®åˆ†æ”¯æ•°é‡ æ·»åŠ  ä¸åŒæ ¸çš„å·ç§¯æ“ä½œ
        for i in range(M):
            # ä¸ºæé«˜æ•ˆç‡ï¼ŒåŸè®ºæ–‡ä¸­ æ‰©å¼ å·ç§¯5x5ä¸º ï¼ˆ3X3ï¼Œdilation=2ï¼‰æ¥ä»£æ›¿ã€‚ ä¸”è®ºæ–‡ä¸­å»ºè®®ç»„å·ç§¯G=32
            self.conv.append(nn.Sequential(
                nn.Conv2d(in_channels, out_channels, 3, stride, padding=1 + i, dilation=1 + i, groups=32, bias=False),
                nn.BatchNorm2d(out_channels),
                nn.ReLU(inplace=True)))
        self.global_pool = nn.AdaptiveAvgPool2d(output_size=1)  # è‡ªé€‚åº”poolåˆ°æŒ‡å®šç»´åº¦    è¿™é‡ŒæŒ‡å®šä¸º1ï¼Œå®ç° GAP
        self.fc1 = nn.Sequential(nn.Conv2d(out_channels, d, 1, bias=False),
                                 nn.BatchNorm2d(d),
                                 nn.ReLU(inplace=True))  # é™ç»´
        self.fc2 = nn.Conv2d(d, out_channels * M * 2, 1, 1, bias=False)  # å‡ç»´
        self.softmax = nn.Softmax(dim=1)  # æŒ‡å®šdim=1  ä½¿å¾—ä¸¤ä¸ªå…¨è¿æ¥å±‚å¯¹åº”ä½ç½®è¿›è¡Œsoftmax,ä¿è¯ å¯¹åº”ä½ç½®a+b+..=1

    def forward(self, input, input2):
        batch_size = input.size(0)
        # batch_size2 = input2.size(0)
        output = []
        # the part of split
        for i, conv in enumerate(self.conv):
            # print(i,conv(input).size())
            output.append(conv(input))  # [batch_size,out_channels,H,W]
            output.append(conv(input2))  # [batch_size,out_channels,H,W]
        # the part of fusion
        U = reduce(lambda x, y: x + y, output)  # é€å…ƒç´ ç›¸åŠ ç”Ÿæˆ æ··åˆç‰¹å¾U  [batch_size,channel,H,W]
        s = self.global_pool(U)  # [batch_size,channel,1,1]
        z = self.fc1(s)  # S->Zé™ç»´   # [batch_size,d,1,1]
        a_b = self.fc2(z)  # Z->aï¼Œb å‡ç»´  è®ºæ–‡ä½¿ç”¨conv 1x1è¡¨ç¤ºå…¨è¿æ¥ã€‚ç»“æœä¸­å‰ä¸€åŠé€šé“å€¼ä¸ºa,åä¸€åŠä¸ºb   [batch_size,out_channels*M,1,1]
        a_b = a_b.reshape(batch_size, self.M * 2, self.out_channels,
                          -1)  # è°ƒæ•´å½¢çŠ¶ï¼Œå˜ä¸º ä¸¤ä¸ªå…¨è¿æ¥å±‚çš„å€¼[batch_size,M,out_channels,1]
        a_b = self.softmax(a_b)  # ä½¿å¾—ä¸¤ä¸ªå…¨è¿æ¥å±‚å¯¹åº”ä½ç½®è¿›è¡Œsoftmax [batch_size,M,out_channels,1]
        # the part of selection
        a_b = list(a_b.chunk(self.M * 2,
                             dim=1))  # split to a and b   chunkä¸ºpytorchæ–¹æ³•ï¼Œå°†tensoræŒ‰ç…§æŒ‡å®šç»´åº¦åˆ‡åˆ†æˆ å‡ ä¸ªtensorå— [[batch_size,1,out_channels,1],[batch_size,1,out_channels,1]
        a_b = list(map(lambda x: x.reshape(batch_size, self.out_channels, 1, 1),
                       a_b))  # å°†æ‰€æœ‰åˆ†å—  è°ƒæ•´å½¢çŠ¶ï¼Œå³æ‰©å±•ä¸¤ç»´  [[batch_size,out_channels,1,1],[batch_size,out_channels,1,1]
        # print("æƒé‡",a_b)
        V = list(map(lambda x, y: x * y, output,
                     a_b))  # æƒé‡ä¸å¯¹åº”  ä¸åŒå·ç§¯æ ¸è¾“å‡ºçš„U é€å…ƒç´ ç›¸ä¹˜[batch_size,out_channels,H,W] * [batch_size,out_channels,1,1] = [batch_size,out_channels,H,W]
        V = reduce(lambda x, y: x + y,
                   V)  # ä¸¤ä¸ªåŠ æƒåçš„ç‰¹å¾ é€å…ƒç´ ç›¸åŠ   [batch_size,out_channels,H,W] + [batch_size,out_channels,H,W] = [batch_size,out_channels,H,W]
        return V  # [batch_size,out_channels,H,W]


################## SKConv
from functools import reduce


class SKConv(nn.Module):
    def __init__(self, in_channels, out_channels, stride=1, M=2, r=16, L=32):
        '''
        :param in_channels:  è¾“å…¥é€šé“ç»´åº¦
        :param out_channels: è¾“å‡ºé€šé“ç»´åº¦   åŸè®ºæ–‡ä¸­ è¾“å…¥è¾“å‡ºé€šé“ç»´åº¦ç›¸åŒ
        :param stride:  æ­¥é•¿ï¼Œé»˜è®¤ä¸º1
        :param M:  åˆ†æ”¯æ•°
        :param r: ç‰¹å¾Zçš„é•¿åº¦ï¼Œè®¡ç®—å…¶ç»´åº¦d æ—¶æ‰€éœ€çš„æ¯”ç‡ï¼ˆè®ºæ–‡ä¸­ ç‰¹å¾S->Z æ˜¯é™ç»´ï¼Œæ•…éœ€è¦è§„å®š é™ç»´çš„ä¸‹ç•Œï¼‰
        :param L:  è®ºæ–‡ä¸­è§„å®šç‰¹å¾Zçš„ä¸‹ç•Œï¼Œé»˜è®¤ä¸º32
        é‡‡ç”¨åˆ†ç»„å·ç§¯ï¼š groups = 32,æ‰€ä»¥è¾“å…¥channelçš„æ•°å€¼å¿…é¡»æ˜¯groupçš„æ•´æ•°å€
        '''
        super(SKConv, self).__init__()
        d = max(in_channels // r, L)  # è®¡ç®—ä»å‘é‡Cé™ç»´åˆ° å‘é‡Z çš„é•¿åº¦d
        self.M = M
        self.out_channels = out_channels
        self.conv = nn.ModuleList()  # æ ¹æ®åˆ†æ”¯æ•°é‡ æ·»åŠ  ä¸åŒæ ¸çš„å·ç§¯æ“ä½œ
        for i in range(M):
            # ä¸ºæé«˜æ•ˆç‡ï¼ŒåŸè®ºæ–‡ä¸­ æ‰©å¼ å·ç§¯5x5ä¸º ï¼ˆ3X3ï¼Œdilation=2ï¼‰æ¥ä»£æ›¿ã€‚ ä¸”è®ºæ–‡ä¸­å»ºè®®ç»„å·ç§¯G=32
            self.conv.append(nn.Sequential(
                nn.Conv2d(in_channels, out_channels, 3, stride, padding=1 + i, dilation=1 + i, groups=32, bias=False),
                nn.BatchNorm2d(out_channels),
                nn.ReLU(inplace=True)))
        self.global_pool = nn.AdaptiveAvgPool2d(output_size=1)  # è‡ªé€‚åº”poolåˆ°æŒ‡å®šç»´åº¦    è¿™é‡ŒæŒ‡å®šä¸º1ï¼Œå®ç° GAP
        self.fc1 = nn.Sequential(nn.Conv2d(out_channels, d, 1, bias=False),
                                 nn.BatchNorm2d(d),
                                 nn.ReLU(inplace=True))  # é™ç»´
        self.fc2 = nn.Conv2d(d, out_channels * M, 1, 1, bias=False)  # å‡ç»´
        self.softmax = nn.Softmax(dim=1)  # æŒ‡å®šdim=1  ä½¿å¾—ä¸¤ä¸ªå…¨è¿æ¥å±‚å¯¹åº”ä½ç½®è¿›è¡Œsoftmax,ä¿è¯ å¯¹åº”ä½ç½®a+b+..=1

    def forward(self, input):
        batch_size = input.size(0)
        output = []
        # the part of split
        for i, conv in enumerate(self.conv):
            # print(i,conv(input).size())
            output.append(conv(input))  # [batch_size,out_channels,H,W]

        # the part of fusion
        U = reduce(lambda x, y: x + y, output)  # é€å…ƒç´ ç›¸åŠ ç”Ÿæˆ æ··åˆç‰¹å¾U  [batch_size,channel,H,W]
        # print(U.size())
        s = self.global_pool(U)  # [batch_size,channel,1,1]
        # print(s.size())
        z = self.fc1(s)  # S->Zé™ç»´   # [batch_size,d,1,1]
        # print(z.size())
        a_b = self.fc2(z)  # Z->aï¼Œb å‡ç»´  è®ºæ–‡ä½¿ç”¨conv 1x1è¡¨ç¤ºå…¨è¿æ¥ã€‚ç»“æœä¸­å‰ä¸€åŠé€šé“å€¼ä¸ºa,åä¸€åŠä¸ºb   [batch_size,out_channels*M,1,1]
        # print(a_b.size())
        a_b = a_b.reshape(batch_size, self.M, self.out_channels, -1)  # è°ƒæ•´å½¢çŠ¶ï¼Œå˜ä¸º ä¸¤ä¸ªå…¨è¿æ¥å±‚çš„å€¼[batch_size,M,out_channels,1]
        # print(a_b.size())
        a_b = self.softmax(a_b)  # ä½¿å¾—ä¸¤ä¸ªå…¨è¿æ¥å±‚å¯¹åº”ä½ç½®è¿›è¡Œsoftmax [batch_size,M,out_channels,1]

        # the part of selection
        a_b = list(a_b.chunk(self.M,
                             dim=1))  # split to a and b   chunkä¸ºpytorchæ–¹æ³•ï¼Œå°†tensoræŒ‰ç…§æŒ‡å®šç»´åº¦åˆ‡åˆ†æˆ å‡ ä¸ªtensorå— [[batch_size,1,out_channels,1],[batch_size,1,out_channels,1]
        # print(a_b[0].size())
        # print(a_b[1].size())
        a_b = list(map(lambda x: x.reshape(batch_size, self.out_channels, 1, 1),
                       a_b))  # å°†æ‰€æœ‰åˆ†å—  è°ƒæ•´å½¢çŠ¶ï¼Œå³æ‰©å±•ä¸¤ç»´  [[batch_size,out_channels,1,1],[batch_size,out_channels,1,1]
        print("a_b", a_b)
        V = list(map(lambda x, y: x * y, output,
                     a_b))  # æƒé‡ä¸å¯¹åº”  ä¸åŒå·ç§¯æ ¸è¾“å‡ºçš„U é€å…ƒç´ ç›¸ä¹˜[batch_size,out_channels,H,W] * [batch_size,out_channels,1,1] = [batch_size,out_channels,H,W]
        V = reduce(lambda x, y: x + y,
                   V)  # ä¸¤ä¸ªåŠ æƒåçš„ç‰¹å¾ é€å…ƒç´ ç›¸åŠ   [batch_size,out_channels,H,W] + [batch_size,out_channels,H,W] = [batch_size,out_channels,H,W]
        return V  # [batch_size,out_channels,H,W]


# ï¼ˆ1ï¼‰é€šé“æ³¨æ„åŠ›æœºåˆ¶
class channel_attention(nn.Module):
    # åˆå§‹åŒ–, in_channelä»£è¡¨è¾“å…¥ç‰¹å¾å›¾çš„é€šé“æ•°, ratioä»£è¡¨ç¬¬ä¸€ä¸ªå…¨è¿æ¥çš„é€šé“ä¸‹é™å€æ•°
    def __init__(self, in_channel, ratio=4):
        # ç»§æ‰¿çˆ¶ç±»åˆå§‹åŒ–æ–¹æ³•
        super(channel_attention, self).__init__()

        # å…¨å±€æœ€å¤§æ± åŒ– [b,c,h,w]==>[b,c,1,1]
        self.max_pool = nn.AdaptiveMaxPool2d(output_size=1)
        # å…¨å±€å¹³å‡æ± åŒ– [b,c,h,w]==>[b,c,1,1]
        self.avg_pool = nn.AdaptiveAvgPool2d(output_size=1)

        # ç¬¬ä¸€ä¸ªå…¨è¿æ¥å±‚, é€šé“æ•°ä¸‹é™4å€
        self.fc1 = nn.Linear(in_features=in_channel, out_features=in_channel // ratio, bias=False)
        # ç¬¬äºŒä¸ªå…¨è¿æ¥å±‚, æ¢å¤é€šé“æ•°
        self.fc2 = nn.Linear(in_features=in_channel // ratio, out_features=in_channel, bias=False)

        # reluæ¿€æ´»å‡½æ•°
        self.relu = nn.ReLU()
        # sigmoidæ¿€æ´»å‡½æ•°
        self.sigmoid = nn.Sigmoid()

    # å‰å‘ä¼ æ’­
    def forward(self, inputs):
        # è·å–è¾“å…¥ç‰¹å¾å›¾çš„shape
        b, c, h, w = inputs.shape

        # è¾“å…¥å›¾åƒåšå…¨å±€æœ€å¤§æ± åŒ– [b,c,h,w]==>[b,c,1,1]
        max_pool = self.max_pool(inputs)
        # è¾“å…¥å›¾åƒçš„å…¨å±€å¹³å‡æ± åŒ– [b,c,h,w]==>[b,c,1,1]
        avg_pool = self.avg_pool(inputs)

        # è°ƒæ•´æ± åŒ–ç»“æœçš„ç»´åº¦ [b,c,1,1]==>[b,c]
        max_pool = max_pool.view([b, c])
        avg_pool = avg_pool.view([b, c])

        # ç¬¬ä¸€ä¸ªå…¨è¿æ¥å±‚ä¸‹é™é€šé“æ•° [b,c]==>[b,c//4]
        x_maxpool = self.fc1(max_pool)
        x_avgpool = self.fc1(avg_pool)

        # æ¿€æ´»å‡½æ•°
        x_maxpool = self.relu(x_maxpool)
        x_avgpool = self.relu(x_avgpool)

        # ç¬¬äºŒä¸ªå…¨è¿æ¥å±‚æ¢å¤é€šé“æ•° [b,c//4]==>[b,c]
        x_maxpool = self.fc2(x_maxpool)
        x_avgpool = self.fc2(x_avgpool)

        # å°†è¿™ä¸¤ç§æ± åŒ–ç»“æœç›¸åŠ  [b,c]==>[b,c]
        x = x_maxpool + x_avgpool
        # sigmoidå‡½æ•°æƒå€¼å½’ä¸€åŒ–
        x = self.sigmoid(x)
        # print("é€šé“ä¸Šæƒé‡", x.shape)
        # è°ƒæ•´ç»´åº¦ [b,c]==>[b,c,1,1]
        x = x.view([b, c, 1, 1])
        # è¾“å…¥ç‰¹å¾å›¾å’Œé€šé“æƒé‡ç›¸ä¹˜ [b,c,h,w]
        outputs = inputs * x

        return outputs


# ---------------------------------------------------- #
# ï¼ˆ2ï¼‰ç©ºé—´æ³¨æ„åŠ›æœºåˆ¶
class spatial_attention(nn.Module):
    # åˆå§‹åŒ–ï¼Œå·ç§¯æ ¸å¤§å°ä¸º7*7
    def __init__(self, kernel_size=7):
        # ç»§æ‰¿çˆ¶ç±»åˆå§‹åŒ–æ–¹æ³•
        super(spatial_attention, self).__init__()

        # ä¸ºäº†ä¿æŒå·ç§¯å‰åçš„ç‰¹å¾å›¾shapeç›¸åŒï¼Œå·ç§¯æ—¶éœ€è¦padding
        padding = kernel_size // 2
        # 7*7å·ç§¯èåˆé€šé“ä¿¡æ¯ [b,2,h,w]==>[b,1,h,w]
        self.conv = nn.Conv2d(in_channels=2, out_channels=1, kernel_size=kernel_size,
                              padding=padding, bias=False)
        # sigmoidå‡½æ•°
        self.sigmoid = nn.Sigmoid()

    # å‰å‘ä¼ æ’­
    def forward(self, inputs):
        # åœ¨é€šé“ç»´åº¦ä¸Šæœ€å¤§æ± åŒ– [b,1,h,w]  keepdimä¿ç•™åŸæœ‰æ·±åº¦
        # è¿”å›å€¼æ˜¯åœ¨æŸç»´åº¦çš„æœ€å¤§å€¼å’Œå¯¹åº”çš„ç´¢å¼•
        x_maxpool, _ = torch.max(inputs, dim=1, keepdim=True)

        # åœ¨é€šé“ç»´åº¦ä¸Šå¹³å‡æ± åŒ– [b,1,h,w]
        x_avgpool = torch.mean(inputs, dim=1, keepdim=True)
        # æ± åŒ–åçš„ç»“æœåœ¨é€šé“ç»´åº¦ä¸Šå †å  [b,2,h,w]
        x = torch.cat([x_maxpool, x_avgpool], dim=1)

        # å·ç§¯èåˆé€šé“ä¿¡æ¯ [b,2,h,w]==>[b,1,h,w]
        x = self.conv(x)
        # ç©ºé—´æƒé‡å½’ä¸€åŒ–
        x = self.sigmoid(x)
        # print("ç©ºé—´ä¸Šæƒé‡", x.shape)
        # è¾“å…¥ç‰¹å¾å›¾å’Œç©ºé—´æƒé‡ç›¸ä¹˜
        outputs = inputs * x

        return outputs


# ---------------------------------------------------- #
# ï¼ˆ3ï¼‰CBAMæ³¨æ„åŠ›æœºåˆ¶
class cbam(nn.Module):
    # åˆå§‹åŒ–ï¼Œin_channelå’Œratio=4ä»£è¡¨é€šé“æ³¨æ„åŠ›æœºåˆ¶çš„è¾“å…¥é€šé“æ•°å’Œç¬¬ä¸€ä¸ªå…¨è¿æ¥ä¸‹é™çš„é€šé“æ•°
    # kernel_sizeä»£è¡¨ç©ºé—´æ³¨æ„åŠ›æœºåˆ¶çš„å·ç§¯æ ¸å¤§å°
    def __init__(self, in_channel, ratio=4, kernel_size=7):
        # ç»§æ‰¿çˆ¶ç±»åˆå§‹åŒ–æ–¹æ³•
        super(cbam, self).__init__()

        # å®ä¾‹åŒ–é€šé“æ³¨æ„åŠ›æœºåˆ¶
        self.channel_attention = channel_attention(in_channel=in_channel, ratio=ratio)
        # å®ä¾‹åŒ–ç©ºé—´æ³¨æ„åŠ›æœºåˆ¶
        self.spatial_attention = spatial_attention(kernel_size=kernel_size)

    # å‰å‘ä¼ æ’­
    def forward(self, inputs):
        # å…ˆå°†è¾“å…¥å›¾åƒç»è¿‡é€šé“æ³¨æ„åŠ›æœºåˆ¶
        x = self.channel_attention(inputs)
        # ç„¶åç»è¿‡ç©ºé—´æ³¨æ„åŠ›æœºåˆ¶
        x = self.spatial_attention(x)
        return x


# æ®‹å·®+SEç»“æœ
class SEBottleneck(nn.Module):
    expansion = 4

    def __init__(self, inplanes, planes, stride=1, downsample=None, reduction=4):  # inplanes, planesè¾“å…¥è¾“å‡ºé€šé“æ•°
        super(SEBottleneck, self).__init__()

        self.firconv = nn.Sequential(
            nn.Conv2d(inplanes, planes, kernel_size=1, bias=False),
            nn.BatchNorm2d(planes),
        )
        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)
        self.bn1 = nn.BatchNorm2d(planes)
        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)
        self.bn1 = nn.BatchNorm2d(planes)
        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,
                               padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(planes)
        self.conv3 = nn.Conv2d(planes, planes * 1, kernel_size=1, bias=False)
        self.bn3 = nn.BatchNorm2d(planes * 1)
        self.relu = nn.ReLU(inplace=True)
        self.se = se_block(planes * 1, reduction)
        # self.se = cbam(planes * 1, ratio=4, kernel_size=7)
        self.downsample = downsample
        self.stride = stride

    def forward(self, x):
        # å…ˆç»è¿‡ä¸€å±‚1*1å·ç§¯
        out_x = self.firconv(x)
        residual = out_x

        # print(x.shape)
        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)
        out = self.conv2(out)
        out = self.bn2(out)
        out = self.relu(out)
        out = self.conv3(out)
        out = self.bn3(out)
        out = self.se(out)
        if self.downsample is not None:
            residual = self.downsample(x)
        #     print("ä¸‹é‡‡æ ·å›¾åƒå¤§å°", residual)
        # print("é€šè¿‡æ³¨æ„åŠ›æœºåˆ¶å¤§å°", out.shape)
        out += residual
        out = self.relu(out)
        return out


# æ®‹å·®+SEç»“æœ
class CABMBottleneck(nn.Module):
    expansion = 4

    def __init__(self, inplanes, planes, stride=1, downsample=None, reduction=4):  # inplanes, planesè¾“å…¥è¾“å‡ºé€šé“æ•°
        super(CABMBottleneck, self).__init__()

        self.firconv = nn.Sequential(
            nn.Conv2d(inplanes, planes, kernel_size=1, bias=False),
            nn.BatchNorm2d(planes),
        )
        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)
        self.bn1 = nn.BatchNorm2d(planes)
        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)
        self.bn1 = nn.BatchNorm2d(planes)
        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,
                               padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(planes)
        self.conv3 = nn.Conv2d(planes, planes * 1, kernel_size=1, bias=False)
        self.bn3 = nn.BatchNorm2d(planes * 1)
        self.relu = nn.ReLU(inplace=True)
        # self.se = se_block(planes * 1, reduction)
        self.cbam = cbam(planes * 1, ratio=4, kernel_size=7)
        self.downsample = downsample
        self.stride = stride

    def forward(self, x):
        # å…ˆç»è¿‡ä¸€å±‚1*1å·ç§¯
        out_x = self.firconv(x)
        residual = out_x

        # print(x.shape)
        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)
        out = self.conv2(out)
        out = self.bn2(out)
        out = self.relu(out)
        out = self.conv3(out)
        out = self.bn3(out)
        # print("seå‰", out.shape)
        out = self.cbam(out)
        # print("seå", out.shape)
        if self.downsample is not None:
            residual = self.downsample(x)
        #     print("ä¸‹é‡‡æ ·å›¾åƒå¤§å°", residual)
        # print("é€šè¿‡æ³¨æ„åŠ›æœºåˆ¶å¤§å°", out.shape)
        out += residual
        out = self.relu(out)
        return out


# æ®‹å·®+SEç»“æœ
class EIR(nn.Module):
    expansion = 4

    def __init__(self, inplanes, planes, middle_channel=16, stride=1, downsample=None,
                 reduction=16):  # inplanes, planesè¾“å…¥è¾“å‡ºé€šé“æ•°
        super(EIR, self).__init__()

        self.firconv = nn.Sequential(
            nn.Conv2d(inplanes, middle_channel, kernel_size=1, bias=False),
            nn.BatchNorm2d(middle_channel),
        )
        self.SEblock = SEBottleneck(inplanes=middle_channel, planes=planes, stride=stride, downsample=downsample,
                                    reduction=reduction)

    def forward(self, x):
        # å…ˆç»è¿‡ä¸€å±‚1*1å·ç§¯
        out_x = self.firconv(x)
        # print(out_x.shape)
        output = self.SEblock(out_x)
        return output


# æ®‹å·®+SEç»“æœ
class EIR_cabm(nn.Module):
    expansion = 4

    def __init__(self, inplanes, planes, middle_channel=16, stride=1, downsample=None,
                 reduction=16):  # inplanes, planesè¾“å…¥è¾“å‡ºé€šé“æ•°
        super(EIR_cabm, self).__init__()

        self.firconv = nn.Sequential(
            nn.Conv2d(inplanes, middle_channel, kernel_size=1, bias=False),
            nn.BatchNorm2d(middle_channel),
        )
        self.CABMblock = CABMBottleneck(inplanes=middle_channel, planes=planes, stride=stride, downsample=downsample,
                                        reduction=reduction)

    def forward(self, x):
        # å…ˆç»è¿‡ä¸€å±‚1*1å·ç§¯
        out_x = self.firconv(x)
        # print(out_x.shape)
        output = self.CABMblock(out_x)
        return output


# ç»“åˆBiFPN è®¾ç½®å¯å­¦ä¹ å‚æ•° å­¦ä¹ ä¸åŒåˆ†æ”¯çš„æƒé‡
# ä¸¤ä¸ªåˆ†æ”¯addæ“ä½œ
class BiFPN_Add2(nn.Module):
    def __init__(self, c1, c2):
        super(BiFPN_Add2, self).__init__()
        # è®¾ç½®å¯å­¦ä¹ å‚æ•° nn.Parameterçš„ä½œç”¨æ˜¯ï¼šå°†ä¸€ä¸ªä¸å¯è®­ç»ƒçš„ç±»å‹Tensorè½¬æ¢æˆå¯ä»¥è®­ç»ƒçš„ç±»å‹parameter
        # å¹¶ä¸”ä¼šå‘å®¿ä¸»æ¨¡å‹æ³¨å†Œè¯¥å‚æ•° æˆä¸ºå…¶ä¸€éƒ¨åˆ† å³model.parameters()ä¼šåŒ…å«è¿™ä¸ªparameter
        # ä»è€Œåœ¨å‚æ•°ä¼˜åŒ–çš„æ—¶å€™å¯ä»¥è‡ªåŠ¨ä¸€èµ·ä¼˜åŒ–
        self.w = nn.Parameter(torch.ones(2, dtype=torch.float32), requires_grad=True)
        self.epsilon = 0.0001
        self.conv = nn.Conv2d(c1, c2, kernel_size=1, stride=1, padding=0)
        self.silu = nn.SiLU()

    def forward(self, x):
        w = self.w
        weight = w / (torch.sum(w, dim=0) + self.epsilon)
        return self.conv(self.silu(weight[0] * x[0] + weight[1] * x[1]))


# ä¸‰ä¸ªåˆ†æ”¯addæ“ä½œ
class BiFPN_Add3(nn.Module):
    def __init__(self, c1, c2):
        super(BiFPN_Add3, self).__init__()
        self.w = nn.Parameter(torch.ones(3, dtype=torch.float32), requires_grad=True)
        self.epsilon = 0.0001
        self.conv = nn.Conv2d(c1, c2, kernel_size=1, stride=1, padding=0)
        self.silu = nn.SiLU()

    def forward(self, x):
        w = self.w
        weight = w / (torch.sum(w, dim=0) + self.epsilon)  # å°†æƒé‡è¿›è¡Œå½’ä¸€åŒ–
        # Fast normalized fusion
        return self.conv(self.silu(weight[0] * x[0] + weight[1] * x[1] + weight[2] * x[2]))


# ç»“åˆBiFPN è®¾ç½®å¯å­¦ä¹ å‚æ•° å­¦ä¹ ä¸åŒåˆ†æ”¯çš„æƒé‡
# ä¸¤ä¸ªåˆ†æ”¯concatæ“ä½œ
class BiFPN_Concat2(nn.Module):
    def __init__(self, dimension=1):
        super(BiFPN_Concat2, self).__init__()
        self.d = dimension
        self.w = nn.Parameter(torch.ones(2, dtype=torch.float32), requires_grad=True)
        self.epsilon = 0.0001

    def forward(self, x):
        # print("BiFPN_Concat2",x.shape)
        w = self.w
        weight = w / (torch.sum(w, dim=0) + self.epsilon)  # å°†æƒé‡è¿›è¡Œå½’ä¸€åŒ–
        # Fast normalized fusion
        x = [weight[0] * x[0], weight[1] * x[1]]
        return torch.cat(x, self.d)


# ä¸‰ä¸ªåˆ†æ”¯concatæ“ä½œ
class BiFPN_Concat3(nn.Module):
    def __init__(self, dimension=1):
        super(BiFPN_Concat3, self).__init__()
        self.d = dimension
        # è®¾ç½®å¯å­¦ä¹ å‚æ•° nn.Parameterçš„ä½œç”¨æ˜¯ï¼šå°†ä¸€ä¸ªä¸å¯è®­ç»ƒçš„ç±»å‹Tensorè½¬æ¢æˆå¯ä»¥è®­ç»ƒçš„ç±»å‹parameter
        # å¹¶ä¸”ä¼šå‘å®¿ä¸»æ¨¡å‹æ³¨å†Œè¯¥å‚æ•° æˆä¸ºå…¶ä¸€éƒ¨åˆ† å³model.parameters()ä¼šåŒ…å«è¿™ä¸ªparameter
        # ä»è€Œåœ¨å‚æ•°ä¼˜åŒ–çš„æ—¶å€™å¯ä»¥è‡ªåŠ¨ä¸€èµ·ä¼˜åŒ–
        self.w = nn.Parameter(torch.ones(3, dtype=torch.float32), requires_grad=True)
        self.epsilon = 0.0001

    def forward(self, x):
        w = self.w
        weight = w / (torch.sum(w, dim=0) + self.epsilon)  # å°†æƒé‡è¿›è¡Œå½’ä¸€åŒ–
        # Fast normalized fusion
        x = [weight[0] * x[0], weight[1] * x[1], weight[2] * x[2]]
        return torch.cat(x, self.d)


###############  AFF
class AFF(nn.Module):
    '''
    å¤šç‰¹å¾èåˆ AFF
    '''

    def __init__(self, channels=64, r=4):
        super().__init__()
        inter_channels = int(channels // r)

        self.local_att = nn.Sequential(
            nn.Conv2d(channels, inter_channels, kernel_size=1, stride=1, padding=0),
            nn.BatchNorm2d(inter_channels),
            nn.ReLU(inplace=True),
            nn.Conv2d(inter_channels, channels, kernel_size=1, stride=1, padding=0),
            nn.BatchNorm2d(channels),
        )
        self.global_att = nn.Sequential(
            nn.AdaptiveAvgPool2d(1),
            nn.Conv2d(channels, inter_channels, kernel_size=1, stride=1, padding=0),
            nn.BatchNorm2d(inter_channels),
            nn.ReLU(inplace=True),
            nn.Conv2d(inter_channels, channels, kernel_size=1, stride=1, padding=0),
            nn.BatchNorm2d(channels),
        )
        # self.avg_pool = nn.AdaptiveAvgPool2d(1)
        # self.conv1 = nn.Conv2d(channels, inter_channels, kernel_size=1, stride=1, padding=0)
        # self.bn1 = nn.BatchNorm2d(inter_channels)
        # self.relu = nn.ReLU(inplace=True)
        # self.conv2 = nn.Conv2d(inter_channels, channels, kernel_size=1, stride=1, padding=0)
        # self.bn2 = nn.BatchNorm2d(channels)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x, residual):
        xa = x + residual
        xl = self.local_att(xa)
        # print("xlå¤§å°", xl.shape)
        xg = self.global_att(xa)
        # xg = self.avg_pool(xa)
        # xg = self.conv1(xg)
        # xg = self.bn1(xg)
        # xg = self.relu(xg)
        # xg = self.conv2(xg)
        # xg = self.bn2(xg)
        xlg = xl + xg
        wei = self.sigmoid(xlg)
        xo = 2 * x * wei + 2 * residual * (1 - wei)
        return xo


###### IAFF
class iAFF(nn.Module):
    '''
    å¤šç‰¹å¾èåˆ iAFF
    '''

    def __init__(self, channels=64, r=4):
        super(iAFF, self).__init__()
        inter_channels = int(channels // r)

        # æœ¬åœ°æ³¨æ„åŠ›
        self.local_att = nn.Sequential(
            nn.Conv2d(channels, inter_channels, kernel_size=1, stride=1, padding=0),
            nn.BatchNorm2d(inter_channels),
            nn.ReLU(inplace=True),
            nn.Conv2d(inter_channels, channels, kernel_size=1, stride=1, padding=0),
            nn.BatchNorm2d(channels),
        )

        # å…¨å±€æ³¨æ„åŠ›
        self.global_att = nn.Sequential(
            nn.AdaptiveAvgPool2d(1),
            nn.Conv2d(channels, inter_channels, kernel_size=1, stride=1, padding=0),
            nn.BatchNorm2d(inter_channels),
            nn.ReLU(inplace=True),
            nn.Conv2d(inter_channels, channels, kernel_size=1, stride=1, padding=0),
            nn.BatchNorm2d(channels),
        )

        # ç¬¬äºŒæ¬¡æœ¬åœ°æ³¨æ„åŠ›
        self.local_att2 = nn.Sequential(
            nn.Conv2d(channels, inter_channels, kernel_size=1, stride=1, padding=0),
            nn.BatchNorm2d(inter_channels),
            nn.ReLU(inplace=True),
            nn.Conv2d(inter_channels, channels, kernel_size=1, stride=1, padding=0),
            nn.BatchNorm2d(channels),
        )
        # ç¬¬äºŒæ¬¡å…¨å±€æ³¨æ„åŠ›
        self.global_att2 = nn.Sequential(
            nn.AdaptiveAvgPool2d(1),
            nn.Conv2d(channels, inter_channels, kernel_size=1, stride=1, padding=0),
            nn.BatchNorm2d(inter_channels),
            nn.ReLU(inplace=True),
            nn.Conv2d(inter_channels, channels, kernel_size=1, stride=1, padding=0),
            nn.BatchNorm2d(channels),
        )

        self.sigmoid = nn.Sigmoid()

    def forward(self, x, residual):  # xä¸€èˆ¬ä¸ºä½å±‚ç‰¹å¾ï¼Œresidualä¸ºé«˜å±‚ç‰¹å¾
        xa = x + residual
        xl = self.local_att(xa)
        xg = self.global_att(xa)
        xlg = xl + xg
        wei = self.sigmoid(xlg)
        # å®ç°çš„æ˜¯ä»€ä¹ˆè‡ªé€‚åº”
        xi = x * wei + residual * (1 - wei)

        xl2 = self.local_att2(xi)
        xg2 = self.global_att(xi)
        xlg2 = xl2 + xg2
        wei2 = self.sigmoid(xlg2)
        xo = x * wei2 + residual * (1 - wei2)
        return xo


"""
æ€ä¹ˆåšåˆ°è‡ªé€‚åº”èåˆæ¨¡å—
åŒ…æ‹¬concatæ¨¡å—
å’Œaddæ¨¡å—
ä»¥åŠæ³¨æ„æœºåˆ¶èåˆæ¨¡å—
"""


# ç¬¬ä¸€ç§è‡ªé€‚åº”èåˆæ–¹å¼ï¼ˆè®¾è®¡é€šé“æ³¨æ„åŠ›æ–¹æ³•ï¼Œè®©ç½‘ç»œå†³å®šè°çš„æƒé‡å¤§ï¼Œè¿›è¡ŒåŠ æƒå¹³å‡ï¼‰
# -------------------------------------------- #
# å®šä¹‰SEæ³¨æ„åŠ›æœºåˆ¶çš„ç±»
class se_block(nn.Module):
    # åˆå§‹åŒ–, in_channelä»£è¡¨è¾“å…¥ç‰¹å¾å›¾çš„é€šé“æ•°, ratioä»£è¡¨ç¬¬ä¸€ä¸ªå…¨è¿æ¥ä¸‹é™é€šé“çš„å€æ•°
    def __init__(self, in_channel, ratio=4):
        # ç»§æ‰¿çˆ¶ç±»åˆå§‹åŒ–æ–¹æ³•
        super(se_block, self).__init__()

        # å±æ€§åˆ†é…
        # å…¨å±€å¹³å‡æ± åŒ–ï¼Œè¾“å‡ºçš„ç‰¹å¾å›¾çš„å®½é«˜=1
        self.avg_pool = nn.AdaptiveAvgPool2d(output_size=1)
        # ç¬¬ä¸€ä¸ªå…¨è¿æ¥å±‚å°†ç‰¹å¾å›¾çš„é€šé“æ•°ä¸‹é™4å€
        self.fc1 = nn.Linear(in_features=in_channel, out_features=in_channel // ratio, bias=False)
        # reluæ¿€æ´»
        self.relu = nn.ReLU()
        # ç¬¬äºŒä¸ªå…¨è¿æ¥å±‚æ¢å¤é€šé“æ•°
        self.fc2 = nn.Linear(in_features=in_channel // ratio, out_features=in_channel, bias=False)
        # sigmoidæ¿€æ´»å‡½æ•°ï¼Œå°†æƒå€¼å½’ä¸€åŒ–åˆ°0-1
        self.sigmoid = nn.Sigmoid()

    # å‰å‘ä¼ æ’­
    def forward(self, inputs):  # inputs ä»£è¡¨è¾“å…¥ç‰¹å¾å›¾

        # è·å–è¾“å…¥ç‰¹å¾å›¾çš„shape
        b, c, h, w = inputs.shape
        # å…¨å±€å¹³å‡æ± åŒ– [b,c,h,w]==>[b,c,1,1]
        x = self.avg_pool(inputs)
        # ç»´åº¦è°ƒæ•´ [b,c,1,1]==>[b,c]
        x = x.view([b, c])

        # ç¬¬ä¸€ä¸ªå…¨è¿æ¥ä¸‹é™é€šé“ [b,c]==>[b,c//4]
        x = self.fc1(x)
        x = self.relu(x)
        # ç¬¬äºŒä¸ªå…¨è¿æ¥ä¸Šå‡é€šé“ [b,c//4]==>[b,c]
        x = self.fc2(x)
        # å¯¹é€šé“æƒé‡å½’ä¸€åŒ–å¤„ç†
        x = self.sigmoid(x)
        # print("æƒé‡è¾“å‡ºx", x.shape)
        # è°ƒæ•´ç»´åº¦ [b,c]==>[b,c,1,1]
        x = x.view([b, c, 1, 1])
        # print("æƒé‡è¾“å‡º2x", x)
        # å°†è¾“å…¥ç‰¹å¾å›¾å’Œé€šé“æƒé‡ç›¸ä¹˜
        # outputs = x * inputs
        return x


# æ®‹å·®+æ³¨æ„åŠ›æœºåˆ¶çš„ç»“æœ
class SEBottleneck(nn.Module):
    expansion = 1

    def __init__(self, inplanes, planes, stride=1, downsample=None, reduction=16):  # inplanes, planesè¾“å…¥è¾“å‡ºé€šé“æ•°
        super(SEBottleneck, self).__init__()
        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)
        self.bn1 = nn.BatchNorm2d(planes)
        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,
                               padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(planes)
        self.conv3 = nn.Conv2d(planes, planes * 1, kernel_size=1, bias=False)
        self.bn3 = nn.BatchNorm2d(planes * 1)
        self.relu = nn.ReLU(inplace=True)
        # æ³¨æ„åŠ›æœºåˆ¶æ·»åŠ ä½ç½®
        self.se = se_block(planes * 1, reduction)
        self.downsample = downsample
        self.stride = stride

    def forward(self, x):
        residual = x

        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)
        out = self.conv2(out)
        out = self.bn2(out)
        out = self.relu(out)
        out = self.conv3(out)
        out = self.bn3(out)
        out = self.se(out)

        if self.downsample is not None:
            residual = self.downsample(x)
        out += residual
        out = self.relu(out)
        return out


# #  åŒæ¨¡æ€ç¼–ç å™¨æå–
# class dual_mode_encoder(nn.Module):
#     # Concatenate a list of tensors along dimension
#     def __init__(self, inplanes, outplanes, stride=1, weight2=1):
#         super().__init__()
#         # æœ€å¤§æ± åŒ–å±‚
#         self.max_pool = nn.AdaptiveMaxPool2d(1)
#         # 1*1å·ç§¯
#         self.conv1 = nn.Conv2d(inplanes, outplanes, kernel_size=1, bias=False)
#         # BNæ“ä½œ
#         self.bn1 = nn.BatchNorm2d(outplanes)
#         self.conv2 = nn.Conv2d(outplanes, outplanes, kernel_size=3, stride=stride,
#                                padding=1, bias=False)
#         self.bn2 = nn.BatchNorm2d(outplanes)
#         self.conv3 = nn.Conv2d(outplanes, outplanes * 4, kernel_size=1, bias=False)
#         self.bn3 = nn.BatchNorm2d(outplanes * 4)
#
#         # self.w1 = weight1
#         self.w2 = weight2
#     def forward(self,x1,x2):
#         return x1+x2

import matplotlib.pyplot as plt
from torchvision import transforms
import os


# ä¿å­˜çš„æ˜¯æ¯ä¸ªé€šé“çš„ç»“æœ
def feature_visualization2(features, feature_num=100, row=10, col=10):
    """
    features: The feature map which you need to visualization
    model_type: The type of feature map
    model_id: The id of feature map
    feature_num: The amount of visualization you need
    ç‰¹å¾ï¼šéœ€è¦å¯è§†åŒ–çš„ç‰¹å¾å›¾
    model_typeï¼šç‰¹å¾å›¾çš„ç±»å‹
    model_idï¼šç‰¹å¾å›¾çš„ id
    feature_numï¼šæ‚¨éœ€è¦çš„å¯è§†åŒ–é‡
    """
    # ä¿å­˜æ–‡ä»¶çš„è·¯å¾„
    save_dir = "features/"
    if not os.path.exists(save_dir):
        os.makedirs(save_dir)

    # print(features.shape)
    # block by channel dimension
    # æ‰“å°æ‹†åˆ†åçš„æ•°ç»„å¤§å°
    # print(features.shape)
    blocks = torch.chunk(features, features.shape[1], dim=1)
    # print(blocks.shape)
    # # size of feature
    # size = features.shape[2], features.shape[3]

    plt.figure()
    # æ¯ä¸ªé€šé“channelç»˜å›¾å¤§axä¸Šä¸€æ¬¡
    for i in range(feature_num):
        torch.squeeze(blocks[i])
        #
        feature = transforms.ToPILImage()(blocks[i].squeeze())
        # print(feature)
        ax = plt.subplot(row, col, i + 1)
        # ax = plt.subplot(int(math.sqrt(feature_num)), int(math.sqrt(feature_num)), i+1) #å‰ä¸¤ä¸ªå‚æ•°m,nå†³å®šäº†å°†ç”»å¸ƒåˆ†ä¸ºmnå—ç¬¬ä¸‰ä¸ªå‚æ•°å†³å®šäº†å½“å‰é€‰ä¸­ç”»å¸ƒçš„ç¼–å·
        ax.set_xticks([])
        ax.set_yticks([])

        plt.imshow(feature)
        # gray feature
        # plt.imshow(feature, cmap='gray')

    # plt.show()
    plt.savefig(save_dir + 'feature_map_2.png', dpi=300)


from torchvision import utils as vutils


# ç¬¬ä¸€ç§è‡ªé€‚åº”èåˆæ–¹å¼ï¼ˆè®¾è®¡é€šé“æ³¨æ„åŠ›æ–¹æ³•ï¼Œè®©ç½‘ç»œå†³å®šè°çš„æƒé‡å¤§ï¼Œé€‰æ‹©æƒé‡å¤§çš„ä¸‰å±‚ç‰¹å¾å±‚ï¼‰
class Add_Adaptive(nn.Module):
    def __init__(self, c1, c2, ratio=4, dimension=1):
        super().__init__()
        # self.w1 = weight1
        # self.w2 = weight2
        self.d = dimension
        self.w = se_block(c1, ratio)
        self.c1 = c1

    def forward(self, x1, x2):
        # vutils.save_image(x1[0], 'features/x1.jpg', normalize=True)
        # vutils.save_image(x2[0], 'features/x2.jpg', normalize=True)
        # print("x1",x1)
        # print("x1çš„å¤§å°", x1.shape)
        # print("x2çš„å¤§å°", x2.shape)
        # å…ˆåˆæˆ6å±‚å›¾åƒ
        x = torch.cat((x1, x2), self.d)
        # print(x.shape)
        # visualize_feature_map(x)
        # é€šè¿‡å…­é€šé“å›¾åƒé€šè¿‡SEé€šé“æ³¨æ„åŠ›ï¼Œä¿ç•™é€‰æ‹©ä¸‰å±‚æƒé‡å¤§çš„é€šé“
        w_x = self.w(x)
        # print(w_x)
        # print(w_x)
        # sorted, index = torch.sort(w_x[0, :, 0, 0], descending=True)
        # å¾—åˆ°ä¸åŒé€šé“çš„æƒé‡
        # å°†å‰ä¸‰ä¸ªæƒé‡å åŠ ï¼Œåä¸‰ä¸ªæƒé‡å åŠ 
        #
        c1 = self.c1
        output_c = (int)(c1 / 2)
        # feature_visualization2(x,feature_num=100, row=10, col=10)
        w1 = torch.sum(w_x[:, 0:output_c, :, :], 1)
        w2 = torch.sum(w_x[:, output_c:c1, :, :], 1)

        # result_x = torch.add(0.5 * x1, 0.5 * x2)

        wv = torch.div(w1, w1 + w2)
        # åœ¨æŸä¸ªdiméƒ¨ä½å¢åŠ ä¸€ä¸ªç»´åº¦
        wv = torch.unsqueeze(wv, dim=1)
        # print("æƒé‡wv", wv)
        x1 = wv * x1
        wc = torch.div(w2, w1 + w2)
        # print("wv",wv)
        wc = torch.unsqueeze(wc, dim=1)
        # print("æƒé‡wc", wc)
        # print("æƒé‡w2", wc.shape)
        x2 = wc * x2

        result_x = torch.add(x1, x2)

        # print("å„å±‚ä»å¤§è¾¾åˆ°å°æ’åˆ—", index)
        # for i in w_x[0,:,0,0]:
        #     print("è¯¥å±‚æƒé‡",i)
        #     print(type(i))
        # print(w_x.shape)

        return result_x


# æ³¨æ„åŠ›æœºåˆ¶æ€ä¹ˆå¤šå±‚èåˆï¼ŒåŒ…æ‹¬å…ˆæå–ç‰¹å¾convç„¶åæ®‹å·®
# é’ˆå¯¹è¿åŠ¨ç‰©ä½“ç‰¹å¾å›¾ï¼ˆä½¿ç”¨ç©ºé—´æ³¨æ„åŠ›æœºåˆ¶ï¼‰
# class SimAM(torch.nn.Module):
#     def __init__(self, e_lambda=1e-4):
#         super(SimAM, self).__init__()
#
#         self.activaton = nn.Sigmoid()
#         self.e_lambda = e_lambda
#
#     def __repr__(self):
#         s = self.__class__.__name__ + '('
#         s += ('lambda=%f)' % self.e_lambda)
#         return s
#
#     @staticmethod
#     def get_module_name():
#         return "simam"
#
#     def forward(self, x):
#         b, c, h, w = x.size()
#
#         n = w * h - 1
#
#         x_minus_mu_square = (x - x.mean(dim=[2, 3], keepdim=True)).pow(2)
#         y = x_minus_mu_square / (4 * (x_minus_mu_square.sum(dim=[2, 3], keepdim=True) / n + self.e_lambda)) + 0.5
#
#         return x * self.activaton(y)


class Add(nn.Module):
    # Concatenate a list of tensors along dimension
    def __init__(self, c1, c2, weight1=1, weight2=1):
        super().__init__()
        self.w1 = weight1
        self.w2 = weight2

    def forward(self, x1, x2):
        return torch.add(self.w1 * x1, self.w2 * x2)


class ChannelAttention(nn.Module):
    def __init__(self, in_planes, ratio=16):
        super(ChannelAttention, self).__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.max_pool = nn.AdaptiveMaxPool2d(1)
        # ä¸€ç»´å·ç§¯æ“ä½œ
        self.f1 = nn.Conv2d(in_planes, in_planes // ratio, 1, bias=False)
        self.relu = nn.ReLU()
        self.f2 = nn.Conv2d(in_planes // ratio, in_planes, 1, bias=False)
        # å†™æ³•äºŒ,äº¦å¯ä½¿ç”¨é¡ºåºå®¹å™¨
        # self.sharedMLP = nn.Sequential(
        # nn.Conv2d(in_planes, in_planes // ratio, 1, bias=False), nn.ReLU(),
        # nn.Conv2d(in_planes // rotio, in_planes, 1, bias=False))

        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        avg_out = self.f2(self.relu(self.f1(self.avg_pool(x))))
        max_out = self.f2(self.relu(self.f1(self.max_pool(x))))
        out = self.sigmoid(avg_out + max_out)
        return out


class SpatialAttention(nn.Module):
    def __init__(self, kernel_size=7):
        super(SpatialAttention, self).__init__()

        assert kernel_size in (3, 7), 'kernel size must be 3 or 7'
        padding = 3 if kernel_size == 7 else 1

        self.conv = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        avg_out = torch.mean(x, dim=1, keepdim=True)
        max_out, _ = torch.max(x, dim=1, keepdim=True)
        x = torch.cat([avg_out, max_out], dim=1)
        x = self.conv(x)
        x1 = torch.mean(x)
        x2 = torch.max(x)
        x = x1 + x2
        # print("x1",x1)
        # print("x2",x2)
        x = self.sigmoid(x)
        return x


##################################################################tyu

class DetectMultiBackend(nn.Module):
    # YOLOv5 MultiBackend class for python inference on various backends
    # YOLOv5 å¤šåç«¯ç±»ï¼Œç”¨äºåœ¨å„ç§åç«¯è¿›è¡Œ python æ¨ç†
    def __init__(self, weights='yolov5s.pt', device=torch.device('cpu'), dnn=False, data=None, fp16=False, fuse=True):
        # Usage:
        #   PyTorch:              weights = *.pt
        #   TorchScript:                    *.torchscript
        #   ONNX Runtime:                   *.onnx
        #   ONNX OpenCV DNN:                *.onnx --dnn
        #   OpenVINO:                       *_openvino_model
        #   CoreML:                         *.mlmodel
        #   TensorRT:                       *.engine
        #   TensorFlow SavedModel:          *_saved_model
        #   TensorFlow GraphDef:            *.pb
        #   TensorFlow Lite:                *.tflite
        #   TensorFlow Edge TPU:            *_edgetpu.tflite
        #   PaddlePaddle:                   *_paddle_model
        from models.experimental import attempt_download, attempt_load  # scoped to avoid circular import

        super().__init__()
        w = str(weights[0] if isinstance(weights, list) else weights)
        # æŸ¥çœ‹æƒé‡æ–‡ä»¶ç±»å‹pt, jit, onnx, xml, engine, coreml, saved_model, pb, tflite, edgetpu, tfjs, paddle, triton
        pt, jit, onnx, xml, engine, coreml, saved_model, pb, tflite, edgetpu, tfjs, paddle, triton = self._model_type(w)
        fp16 &= pt or jit or onnx or engine or triton  # FP16
        nhwc = coreml or saved_model or pb or tflite or edgetpu  # BHWC formats (vs torch BCWH)
        stride = 32  # default stride
        cuda = torch.cuda.is_available() and device.type != 'cpu'  # use CUDA
        if not (pt or triton):
            w = attempt_download(w)  # download if not local

        if pt:  # PyTorch
            model = attempt_load(weights if isinstance(weights, list) else w, device=device, inplace=True, fuse=fuse)
            stride = max(int(model.stride.max()), 32)  # model stride
            names = model.module.names if hasattr(model, 'module') else model.names  # get class names
            model.half() if fp16 else model.float()
            self.model = model  # explicitly assign for to(), cpu(), cuda(), half()

        # class names
        if 'names' not in locals():
            names = yaml_load(data)['names'] if data else {i: f'class{i}' for i in range(999)}
        if names[0] == 'n01440764' and len(names) == 1000:  # ImageNet
            names = yaml_load(ROOT / 'data/ImageNet.yaml')['names']  # human-readable names

        self.__dict__.update(locals())  # assign all variables to self

    # ä¿®æ”¹
    def forward(self, im, im2, augment=False, visualize=False):
        # YOLOv5 MultiBackend inference
        b, ch, h, w = im.shape  # batch, channel, height, width
        if self.fp16 and im.dtype != torch.float16:
            im = im.half()  # to FP16
        if self.nhwc:
            im = im.permute(0, 2, 3, 1)  # torch BCHW to numpy BHWC shape(1,320,192,3)

        if self.pt:  # PyTorch
            y = self.model(im, im2, augment=augment, visualize=visualize) if augment or visualize else self.model(im,
                                                                                                                  im2)

            # y = [x if isinstance(x, np.ndarray) else x.numpy() for x in y]
            # y = [x if isinstance(x, np.ndarray) else x.numpy() for x in y]
            # y[0][..., :4] *= [w, h, w, h]  # xywh normalized to pixels

        if isinstance(y, (list, tuple)):
            return self.from_numpy(y[0]) if len(y) == 1 else [self.from_numpy(x) for x in y]
        else:
            return self.from_numpy(y)

    def from_numpy(self, x):
        return torch.from_numpy(x).to(self.device) if isinstance(x, np.ndarray) else x

    def warmup(self, imgsz=(1, 3, 640, 640)):
        # Warmup model by running inference once
        warmup_types = self.pt, self.jit, self.onnx, self.engine, self.saved_model, self.pb, self.triton
        if any(warmup_types) and (self.device.type != 'cpu' or self.triton):
            im = torch.empty(*imgsz, dtype=torch.half if self.fp16 else torch.float, device=self.device)  # input
            for _ in range(2 if self.jit else 1):  #
                self.forward(im)  # warmup

    @staticmethod
    def _model_type(p='path/to/model.pt'):
        # Return model type from model path, i.e. path='path/to/model.onnx' -> type=onnx
        # types = [pt, jit, onnx, xml, engine, coreml, saved_model, pb, tflite, edgetpu, tfjs, paddle]
        from export import export_formats
        from utils.downloads import is_url
        sf = list(export_formats().Suffix)  # export suffixes
        if not is_url(p, check=False):
            check_suffix(p, sf)  # checks
        url = urlparse(p)  # if url may be Triton inference server
        types = [s in Path(p).name for s in sf]
        types[8] &= not types[9]  # tflite &= not edgetpu
        triton = not any(types) and all([any(s in url.scheme for s in ['http', 'grpc']), url.netloc])
        return types + [triton]

    @staticmethod
    def _load_metadata(f=Path('path/to/meta.yaml')):
        # Load metadata from meta.yaml if it exists
        if f.exists():
            d = yaml_load(f)
            return d['stride'], d['names']  # assign stride, names
        return None, None


class AutoShape(nn.Module):
    # YOLOv5 input-robust model wrapper for passing cv2/np/PIL/torch inputs. Includes preprocessing, inference and NMS
    conf = 0.25  # NMS confidence threshold
    iou = 0.45  # NMS IoU threshold
    agnostic = False  # NMS class-agnostic
    multi_label = False  # NMS multiple labels per box
    classes = None  # (optional list) filter by class, i.e. = [0, 15, 16] for COCO persons, cats and dogs
    max_det = 1000  # maximum number of detections per image
    amp = False  # Automatic Mixed Precision (AMP) inference

    def __init__(self, model, verbose=True):
        super().__init__()
        if verbose:
            LOGGER.info('Adding AutoShape... ')
        copy_attr(self, model, include=('yaml', 'nc', 'hyp', 'names', 'stride', 'abc'), exclude=())  # copy attributes
        self.dmb = isinstance(model, DetectMultiBackend)  # DetectMultiBackend() instance
        self.pt = not self.dmb or model.pt  # PyTorch model
        self.model = model.eval()
        if self.pt:
            m = self.model.model.model[-1] if self.dmb else self.model.model[-1]  # Detect()
            m.inplace = False  # Detect.inplace=False for safe multithread inference
            m.export = True  # do not output loss values

    def _apply(self, fn):
        # Apply to(), cpu(), cuda(), half() to model tensors that are not parameters or registered buffers
        self = super()._apply(fn)
        if self.pt:
            m = self.model.model.model[-1] if self.dmb else self.model.model[-1]  # Detect()
            m.stride = fn(m.stride)
            m.grid = list(map(fn, m.grid))
            if isinstance(m.anchor_grid, list):
                m.anchor_grid = list(map(fn, m.anchor_grid))
        return self

    @smart_inference_mode()
    def forward(self, ims, size=640, augment=False, profile=False):
        # Inference from various sources. For size(height=640, width=1280), RGB images example inputs are:
        #   file:        ims = 'data/images/zidane.jpg'  # str or PosixPath
        #   URI:             = 'https://ultralytics.com/images/zidane.jpg'
        #   OpenCV:          = cv2.imread('image.jpg')[:,:,::-1]  # HWC BGR to RGB x(640,1280,3)
        #   PIL:             = Image.open('image.jpg') or ImageGrab.grab()  # HWC x(640,1280,3)
        #   numpy:           = np.zeros((640,1280,3))  # HWC
        #   torch:           = torch.zeros(16,3,320,640)  # BCHW (scaled to size=640, 0-1 values)
        #   multiple:        = [Image.open('image1.jpg'), Image.open('image2.jpg'), ...]  # list of images

        dt = (Profile(), Profile(), Profile())
        with dt[0]:
            if isinstance(size, int):  # expand
                size = (size, size)
            p = next(self.model.parameters()) if self.pt else torch.empty(1, device=self.model.device)  # param
            autocast = self.amp and (p.device.type != 'cpu')  # Automatic Mixed Precision (AMP) inference
            if isinstance(ims, torch.Tensor):  # torch
                with amp.autocast(autocast):
                    return self.model(ims.to(p.device).type_as(p), augment=augment)  # inference

            # Pre-process
            n, ims = (len(ims), list(ims)) if isinstance(ims, (list, tuple)) else (1, [ims])  # number, list of images
            shape0, shape1, files = [], [], []  # image and inference shapes, filenames
            for i, im in enumerate(ims):
                f = f'image{i}'  # filename
                if isinstance(im, (str, Path)):  # filename or uri
                    im, f = Image.open(requests.get(im, stream=True).raw if str(im).startswith('http') else im), im
                    im = np.asarray(exif_transpose(im))
                elif isinstance(im, Image.Image):  # PIL Image
                    im, f = np.asarray(exif_transpose(im)), getattr(im, 'filename', f) or f
                files.append(Path(f).with_suffix('.jpg').name)
                if im.shape[0] < 5:  # image in CHW
                    im = im.transpose((1, 2, 0))  # reverse dataloader .transpose(2, 0, 1)
                im = im[..., :3] if im.ndim == 3 else cv2.cvtColor(im, cv2.COLOR_GRAY2BGR)  # enforce 3ch input
                s = im.shape[:2]  # HWC
                shape0.append(s)  # image shape
                g = max(size) / max(s)  # gain
                shape1.append([int(y * g) for y in s])
                ims[i] = im if im.data.contiguous else np.ascontiguousarray(im)  # update
            shape1 = [make_divisible(x, self.stride) for x in np.array(shape1).max(0)]  # inf shape
            x = [letterbox(im, shape1, auto=False)[0] for im in ims]  # pad
            x = np.ascontiguousarray(np.array(x).transpose((0, 3, 1, 2)))  # stack and BHWC to BCHW
            x = torch.from_numpy(x).to(p.device).type_as(p) / 255  # uint8 to fp16/32

        with amp.autocast(autocast):
            # Inference
            with dt[1]:
                y = self.model(x, augment=augment)  # forward

            # Post-process
            with dt[2]:
                y = non_max_suppression(y if self.dmb else y[0],
                                        self.conf,
                                        self.iou,
                                        self.classes,
                                        self.agnostic,
                                        self.multi_label,
                                        max_det=self.max_det)  # NMS
                for i in range(n):
                    scale_boxes(shape1, y[i][:, :4], shape0[i])

            return Detections(ims, y, files, dt, self.names, x.shape)


class Detections:
    # YOLOv5 detections class for inference results
    def __init__(self, ims, pred, files, times=(0, 0, 0), names=None, shape=None):
        super().__init__()
        d = pred[0].device  # device
        gn = [torch.tensor([*(im.shape[i] for i in [1, 0, 1, 0]), 1, 1], device=d) for im in ims]  # normalizations
        self.ims = ims  # list of images as numpy arrays
        self.pred = pred  # list of tensors pred[0] = (xyxy, conf, cls)
        self.names = names  # class names
        self.files = files  # image filenames
        self.times = times  # profiling times
        self.xyxy = pred  # xyxy pixels
        self.xywh = [xyxy2xywh(x) for x in pred]  # xywh pixels
        self.xyxyn = [x / g for x, g in zip(self.xyxy, gn)]  # xyxy normalized
        self.xywhn = [x / g for x, g in zip(self.xywh, gn)]  # xywh normalized
        self.n = len(self.pred)  # number of images (batch size)
        self.t = tuple(x.t / self.n * 1E3 for x in times)  # timestamps (ms)
        self.s = tuple(shape)  # inference BCHW shape

    def _run(self, pprint=False, show=False, save=False, crop=False, render=False, labels=True, save_dir=Path('')):
        s, crops = '', []
        for i, (im, pred) in enumerate(zip(self.ims, self.pred)):
            s += f'\nimage {i + 1}/{len(self.pred)}: {im.shape[0]}x{im.shape[1]} '  # string
            if pred.shape[0]:
                for c in pred[:, -1].unique():
                    n = (pred[:, -1] == c).sum()  # detections per class
                    s += f"{n} {self.names[int(c)]}{'s' * (n > 1)}, "  # add to string
                s = s.rstrip(', ')
                if show or save or render or crop:
                    annotator = Annotator(im, example=str(self.names))
                    for *box, conf, cls in reversed(pred):  # xyxy, confidence, class
                        label = f'{self.names[int(cls)]} {conf:.2f}'
                        if crop:
                            file = save_dir / 'crops' / self.names[int(cls)] / self.files[i] if save else None
                            crops.append({
                                'box': box,
                                'conf': conf,
                                'cls': cls,
                                'label': label,
                                'im': save_one_box(box, im, file=file, save=save)})
                        else:  # all others
                            annotator.box_label(box, label if labels else '', color=colors(cls))
                    im = annotator.im
            else:
                s += '(no detections)'

            im = Image.fromarray(im.astype(np.uint8)) if isinstance(im, np.ndarray) else im  # from np
            if show:
                if is_jupyter():
                    from IPython.display import display
                    display(im)
                else:
                    im.show(self.files[i])
            if save:
                f = self.files[i]
                im.save(save_dir / f)  # save
                if i == self.n - 1:
                    LOGGER.info(f"Saved {self.n} image{'s' * (self.n > 1)} to {colorstr('bold', save_dir)}")
            if render:
                self.ims[i] = np.asarray(im)
        if pprint:
            s = s.lstrip('\n')
            return f'{s}\nSpeed: %.1fms pre-process, %.1fms inference, %.1fms NMS per image at shape {self.s}' % self.t
        if crop:
            if save:
                LOGGER.info(f'Saved results to {save_dir}\n')
            return crops

    @TryExcept('Showing images is not supported in this environment')
    def show(self, labels=True):
        self._run(show=True, labels=labels)  # show results

    def save(self, labels=True, save_dir='runs/detect/exp', exist_ok=False):
        save_dir = increment_path(save_dir, exist_ok, mkdir=True)  # increment save_dir
        self._run(save=True, labels=labels, save_dir=save_dir)  # save results

    def crop(self, save=True, save_dir='runs/detect/exp', exist_ok=False):
        save_dir = increment_path(save_dir, exist_ok, mkdir=True) if save else None
        return self._run(crop=True, save=save, save_dir=save_dir)  # crop results

    def render(self, labels=True):
        self._run(render=True, labels=labels)  # render results
        return self.ims

    def pandas(self):
        # return detections as pandas DataFrames, i.e. print(results.pandas().xyxy[0])
        new = copy(self)  # return copy
        ca = 'xmin', 'ymin', 'xmax', 'ymax', 'confidence', 'class', 'name'  # xyxy columns
        cb = 'xcenter', 'ycenter', 'width', 'height', 'confidence', 'class', 'name'  # xywh columns
        for k, c in zip(['xyxy', 'xyxyn', 'xywh', 'xywhn'], [ca, ca, cb, cb]):
            a = [[x[:5] + [int(x[5]), self.names[int(x[5])]] for x in x.tolist()] for x in getattr(self, k)]  # update
            setattr(new, k, [pd.DataFrame(x, columns=c) for x in a])
        return new

    def tolist(self):
        # return a list of Detections objects, i.e. 'for result in results.tolist():'
        r = range(self.n)  # iterable
        x = [Detections([self.ims[i]], [self.pred[i]], [self.files[i]], self.times, self.names, self.s) for i in r]
        # for d in x:
        #    for k in ['ims', 'pred', 'xyxy', 'xyxyn', 'xywh', 'xywhn']:
        #        setattr(d, k, getattr(d, k)[0])  # pop out of list
        return x

    def print(self):
        LOGGER.info(self.__str__())

    def __len__(self):  # override len(results)
        return self.n

    def __str__(self):  # override print(results)
        return self._run(pprint=True)  # print results

    def __repr__(self):
        return f'YOLOv5 {self.__class__} instance\n' + self.__str__()


class Proto(nn.Module):
    # YOLOv5 mask Proto module for segmentation models
    def __init__(self, c1, c_=256, c2=32):  # ch_in, number of protos, number of masks
        super().__init__()
        self.cv1 = Conv(c1, c_, k=3)
        self.upsample = nn.Upsample(scale_factor=2, mode='nearest')
        self.cv2 = Conv(c_, c_, k=3)
        self.cv3 = Conv(c_, c2)

    def forward(self, x):
        return self.cv3(self.cv2(self.upsample(self.cv1(x))))


class Classify(nn.Module):
    # YOLOv5 classification head, i.e. x(b,c1,20,20) to x(b,c2)
    def __init__(self,
                 c1,
                 c2,
                 k=1,
                 s=1,
                 p=None,
                 g=1,
                 dropout_p=0.0):  # ch_in, ch_out, kernel, stride, padding, groups, dropout probability
        super().__init__()
        c_ = 1280  # efficientnet_b0 size
        self.conv = Conv(c1, c_, k, s, autopad(k, p), g)
        self.pool = nn.AdaptiveAvgPool2d(1)  # to x(b,c_,1,1)
        self.drop = nn.Dropout(p=dropout_p, inplace=True)
        self.linear = nn.Linear(c_, c2)  # to x(b,c2)

    def forward(self, x):
        if isinstance(x, list):
            x = torch.cat(x, 1)
        return self.linear(self.drop(self.pool(self.conv(x)).flatten(1)))
